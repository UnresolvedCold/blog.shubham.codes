{
    "status": "ok",
    "feed": {
        "url": "http://blog-schwiftycold.firebaseapp.com/rss.xml",
        "title": "Shubham's Blog",
        "link": "https://blog-schwiftycold.firebaseapp.com/",
        "author": "",
        "description": "Welcome to Shubham's corner on the internet. I write about things I learn and things I find interesting.",
        "image": ""
    },
    "items": [
        {
            "title": "Migrating blogs to Hugo",
            "pubDate": "2025-03-12 11:34:03",
            "link": "https://blog-schwiftycold.firebaseapp.com/blog/2025-03-12-migration-to-hugo/",
            "guid": "https://blog-schwiftycold.firebaseapp.com/blog/2025-03-12-migration-to-hugo/",
            "author": "",
            "thumbnail": "",
            "description": "Migrating my blogs to Hugo",
            "content": "<p>This is just a notification that my new blogs will be posted on the new site.\nThe site URL will still remain the same - blog.shubham.codes</p>",
            "enclosure": {},
            "categories": []
        },
        {
            "title": "Using Jmespath in Emacs",
            "pubDate": "2024-02-05 18:30:00",
            "link": "https://blog-schwiftycold.firebaseapp.com/blog/2024-02-06-jmespath-emacs-library/",
            "guid": "https://blog-schwiftycold.firebaseapp.com/blog/2024-02-06-jmespath-emacs-library/",
            "author": "",
            "thumbnail": "",
            "description": "Querying JSON files/data using Jmespath library in Emacs",
            "content": "\n<h2>Introduction</h2>\n<p>When you have a small JSON file, it is quite easy to look for what you want.\nBut querying a large JSON data is very troublesome.</p>\n<p>This is where people use tools like Jmespath to filter and transform data into their liking.\nThis post is about a small wrapper over <code>jp</code> CLI utility that you can use while working on JSON files in Emacs.</p>\n<h2>Installing JP</h2>\n<h3>Linux</h3>\n<p>On Linux, you can install the utility by first downloding the binary and then installing it.</p>\n<pre><code>&gt; sudo wget https://github.com/jmespath/jp/releases/latest/download/jp-linux-amd64 \\\n&gt;   -O /usr/local/bin/jp  &amp;&amp; sudo chmod +x /usr/local/bin/jp\n</code></pre>\n<h3>Mac</h3>\n<p>On Mac, you can install Jmespath CLI using <code>brew</code>.</p>\n<pre><code>&gt; brew install jmespath/jmespath/jp\n</code></pre>\n<h2>Adding Jmespath recipe in Emacs</h2>\n<p>This is my first recipe that I published on <a href=\"https://melpa.org/\">MELPA</a>.Here are the steps to install it in <code>Doom</code> using <code>straight</code>.\nYou can use any package manager to install it using <code>MELPA</code> repository.</p>\n<h3>Doom Emacs</h3>\n<ol>\n<li>On Doom, you can just mention the below recipe in your <code>package.el</code>.</li>\n</ol>\n<pre><code>&gt; (package! jmespath)\n</code></pre>\n<ol>\n<li>Also, add the below line in your <code>config.el</code>.</li>\n</ol>\n<pre><code>&gt; (use-package jmespath)\n</code></pre>\n<h2>Using Jmespath</h2>\n<p>There is an interactive function, <code>jmespath-query-and-show</code> that you can use to query the currently opened buffer or a file.\nTo use it with current buffer, you can simply call this function and enter your query.\nThe output will be shown on a new buffer with name <strong>JMESPath Result</strong>.</p>\n<p>To use this with a different file, you can set the <code>Universal Argument</code> using <code>C-u</code> or <code>SPC-u</code> (using evil).\nThen you can enter the file to execute the query on.</p>\n<h2>Read more</h2>\n<p>To learn more about JMESPath, visit <a href=\"https://jmespath.org/\">Jamespath page</a>.</p>\n",
            "enclosure": {},
            "categories": []
        },
        {
            "title": "Running PMML models in Erlang using NIF and C++",
            "pubDate": "2023-10-15 05:10:32",
            "link": "https://blog-schwiftycold.firebaseapp.com/blog/2023-10-15-pmml-library-erlang-nif/",
            "guid": "https://blog-schwiftycold.firebaseapp.com/blog/2023-10-15-pmml-library-erlang-nif/",
            "author": "",
            "thumbnail": "",
            "description": "Here, we will use NIF to call the C++ functions on Erlang side and use the cPMML library to parse the PMML files and run prediction on a linear regression model (can be used for any AI/ML model).",
            "content": "\n<h2>Introduction</h2>\n<p>Erlang is a great language for building concurrent systems that are fault-tolerant and scalable.\nBut it lacks some of the libraries that are available in other languages.\nOne such example is using <code>PMML</code> files for machine learning models.\nAt the time of writing, Erlang doesn't have a library for parsing <code>PMML</code> files.\nThis is a problem for people who want to use Erlang for building machine learning systems.\nHere I'll show how to use <code>C++</code> to build a <code>NIF</code> that can be used in Erlang to parse <code>PMML</code> files.\nMore specifically, I'll use the <a href=\"https://github.com/AmadeusITGroup/cPMML\">cPMML</a> library to build a <code>NIF</code> that can be used in Erlang.</p>\n<h2>Erlang NIF</h2>\n<p>Erlang NIF provides you a way to define your functions in <code>C/C++</code> and call them in <code>Erlang</code> code natively.\nThe <code>C/C++</code> program is compiled to generate a library file that can be used in Erlang.\nThis library is dynamically linked to the Erlang VM and is the fastest way of calling the <code>C/C++</code> code from <code>Erlang</code>.\nThe disadvantage of the approach is that if the <code>C/C++</code> code crashes, it will crash the Erlang VM.\nAnd you will need to maintain the <code>C/C++</code> code along with the <code>Erlang</code> code.</p>\n<h2>Prerequisites</h2>\n<p><code>cPMML</code> library requires a version of <code>C++</code> that supports <code>C++11</code> standard.\nAlso, make sure you have the required header files on the <code>Erlang</code> side.</p>\n<p>You can find them on Mac OS or Linux using the find function.\nIf the header files are located in multiple locations, use the <code>Celler</code> one.</p>\n<pre><code>find / -name erl_nif.h | grep erl_nif.h\n</code></pre>\n<h2>Hello Nif program</h2>\n<h3>C/C++ code</h3>\n<h4>Header file</h4>\n<p>You need to include the below header file to use the <code>Erlang</code> functionalities.\nOn a lower level, it defines the data structures and environment that <code>Erlang</code> provides to the <code>C/C++</code> code.</p>\n<pre><code>#include &lt;erl_nif.h&gt;\n</code></pre>\n<h4>Function Definition</h4>\n<p>These functions are called from <code>Erlang</code> code.\nThey must follow a specific structure and return a specific type of value.\nHere, we will define a function that will return a \"Hello, World!\" string.</p>\n<p><code>ERL_NIF_TERM</code> is the return type of the function.\nIt is an interface for various return types like <code>binary</code>, <code>tuple</code>, <code>list</code>, <code>atom</code>, etc.\nMeaning, that you can return any of these types from the function.</p>\n<p><code>ErlNifEnv</code> is the pointer to the Erlang environment.\nIt provides you access to various <code>Erlang</code> functionalities like memory management, exception handling and <code>Erlang</code> term creation.\nFor us, it will help in creating Erlang Terms like <code>string</code>.\n<code>enif_make_string</code> is the function that will create a string from the <code>C/C++</code> code.</p>\n<p>The <code>argc</code> and <code>argv</code> provide the number of arguments and the arguments passed to the function from <code>Erlang</code> code.</p>\n<p>Below is the function definition for <code>hello world</code>.</p>\n<pre><code>static ERL_NIF_TERM hello_world(ErlNifEnv* env, int argc, const ERL_NIF_TERM argv[]) {\n    return enif_make_string(env, \"Hello, World!\", ERL_NIF_LATIN1);\n}\n</code></pre>\n<h4>Export functions</h4>\n<p>You need to specify the functions that you want to export to the <code>Erlang</code> code.\nThe structure is a list of <code>ErlNifFunc</code> objects.\nEach object has the name of the function that Erlang sees, the number of arguments and the function pointer.</p>\n<pre><code>static ErlNifFunc nif_funcs[] = {\n    {\"hello_world\", 0, hello_world}\n};\n\n</code></pre>\n<p>To initialize the <code>NIF</code> library, you need to call the <code>ERL_NIF_INIT</code> function.\nThis takes in the name of your <code>Erlang</code> module and the exported functions.</p>\n<p>Let's call our <code>Erlang</code> module <code>hello_nif</code>.</p>\n<pre><code>ERL_NIF_INIT(hello_nif, nif_funcs, NULL, NULL, NULL, NULL)\n</code></pre>\n<h4>Final code</h4>\n<pre><code>extern \"C\" {\n  #include &lt;erl_nif.h&gt;\n  static ERL_NIF_TERM hello_world(ErlNifEnv* env, int argc, const ERL_NIF_TERM argv[]) {\n      return enif_make_string(env, \"Hello, World!\", ERL_NIF_LATIN1);\n  }\n  static ErlNifFunc nif_funcs[] = {\n      {\"hello_world\", 0, hello_world}\n  };\n}\n\nERL_NIF_INIT(hello_nif, nif_funcs, NULL, NULL, NULL, NULL);\n</code></pre>\n<h3>Erlang code</h3>\n<h4>Erlang module</h4>\n<p>We will name our <code>Erlang</code> module <code>hello_nif.erl</code>.</p>\n<pre><code>-module(hello_nif).\n</code></pre>\n<h4>Define your NIF functions</h4>\n<p>This specifies the functions that are exported from the <code>C/C++</code> code.</p>\n<pre><code>-export([hello_world/0]).\n-nifs([hello_world/0]).\n</code></pre>\n<h4>Load the library on module load</h4>\n<p>If you compiled your <code>C/C++</code> code to a library named <code>hello_nif.so</code>, you can load it using the <code>load_nif</code> function.</p>\n<pre><code>-on_load(init/0).\n\ninit() -&gt;\n    ok = erlang:load_nif(\"./hello_nif\", 0).\n</code></pre>\n<h4>Fallback function</h4>\n<p>The <code>C/C++</code> code can crash and you need to handle that in the <code>Erlang</code> code.\nThese functions will run when the <code>C/C++</code> code crashes.\nYou should name each of these functions with the same name in the <code>C/C++</code> code.</p>\n<pre><code>hello_world() -&gt;\n    exit(nif_library_not_loaded).\n</code></pre>\n<h4>Final code</h4>\n<pre><code>-module(hello_nif).\n-nifs([hello_world/0]).\n-on_load(init/0).\n\ninit() -&gt;\n    ok = erlang:load_nif(\"./hello_nif\", 0).\n\nhello_world() -&gt;\n    exit(nif_library_not_loaded).\n</code></pre>\n<h3>Compiling and Running</h3>\n<p>To compile the <code>C/C++</code> code you can use <code>gcc</code>.</p>\n<h4>Mac OS</h4>\n<pre><code>gcc -o hello_nif.so hello.c -I /usr/local/lib/erlang/erts-13.2.2.2/include/ -bundle -bundle_loader /usr/local/lib/erlang/erts-13.2.2.2/bin/beam.smp\n</code></pre>\n<h4>Linux</h4>\n<pre><code>gcc -o hello_nif.so hello.c -I /usr/lib/erlang/erts-13.2.2.2/include -shared -fpic\n</code></pre>\n<p>On Erlng, you can call the <code>hello_world</code> function.</p>\n<pre><code>&gt; c(hello_nif).\n&gt; hello_nif:hello_world().\n\"Hello, World!\"\n</code></pre>\n<h2>cPMML NIF</h2>\n<p>We saw how to create a simple <code>NIF</code> that returns a string.\nNow, let's create a <code>NIF</code> that can take input and run prediction using a <code>PMML</code> model file.\nFor this, we will use the same model file we created in the <a href=\"https://blog-schwiftycold.firebaseapp.com/blog/2023-10-02-cpmml-predictions\">previous post</a>.</p>\n<p>We have a PMML model for linear regression, <code>y = 2x + 1</code> and we want to predict the value of <code>y</code> for a given value of <code>x</code>.\nKeeping it short, we will expose a function called, <code>predict</code> that will take the input and return the output.</p>\n<h3>C/C++ code</h3>\n<p>Assuming you can compile <code>cPMML</code> files as described in the <a href=\"https://blog-schwiftycold.firebaseapp.com/blog/2023-10-02-cpmml-predictions\">previous post</a>.</p>\n<h4>Header files</h4>\n<pre><code>#include &lt;iostream&gt;\n#include &lt;vector&gt;\n#include &lt;string&gt;\n#include &lt;map&gt;\n#include \"cPMML.h\"\n\nusing namespace std;\n</code></pre>\n<h4>PMML Parser class</h4>\n<p>We want to load the model once and use it for multiple predictions.\nFor this, let's create a class that will load the model and call predictions on it.\nWe will maintain only one instance of this class and use it for all the predictions.</p>\n<pre><code>class PmmlModelParser {\nprivate:\n  cpmml::Model model;\n\npublic:\n    PmmlModelParser(const string&amp; modelname) {\n        model = cpmml::Model(modelname);\n    }\n\n    string predict(const unordered_map&lt;string, string&gt;&amp; x_input) {\n        return model.predict(x_input);\n    }\n};\n\n// Global variable\nPmmlModelParser *pmmlModelParser = nullptr;\n\n</code></pre>\n<h4>NIF implementation</h4>\n<p>We will create two functions, <code>init</code> and <code>predict</code>.\nThe <code>init</code> function will take the <code>PMML</code> file as input and load it.\nThe <code>predict</code> function will take the input and return the output.</p>\n<p>The below code describes the structure of the <code>NIF</code> functions.</p>\n<pre><code>extern \"C\" {\n    #include &lt;erl_nif.h&gt;\n    \n    static ERL_NIF_TERM init(ErlNifEnv* env, int argc, const ERL_NIF_TERM argv[]) {}\n\n    static ERL_NIF_TERM predict(ErlNifEnv* env, int argc, const ERL_NIF_TERM argv[]) {}\n\n    static ErlNifFunc nif_funcs[] = {\n        {\"init\", 1, init},\n        {\"evaluate\", 1, predict}\n    };\n}\n\nERL_NIF_INIT(lr_model, nif_funcs, NULL, NULL, NULL, NULL)\n\n</code></pre>\n<h4>init function</h4>\n<p>The <code>init</code> function will take the <code>PMML</code> file as input and load it.\nWe will use the <code>enif_inspect_binary</code> function to get the <code>PMML</code> file as a binary.\nThen we will convert it to a string and pass it to the <code>PmmlModelParser</code> class.</p>\n<pre><code>static ERL_NIF_TERM init(ErlNifEnv* env, int argc, const ERL_NIF_TERM argv[]) {\n    ErlNifBinary input_bin;\n    if (!enif_inspect_binary(env, argv[0], &amp;input_bin)) {\n        return enif_make_badarg(env);\n    }\n    string input(reinterpret_cast&lt;char*&gt;(input_bin.data), input_bin.size);\n    pmmlModelParser = new PmmlModelParser(input);\n    return enif_make_atom(env, \"ok\");\n}\n</code></pre>\n<h4>predict function</h4>\n<p>The <code>predict</code> function will take the input and return the output.\nThe input will be a string and the output will be a string on the Erlang side.</p>\n<p>But on the <code>C/C++</code> side, we will convert the input to a map and pass it to the <code>PmmlModelParser</code> class because <code>cPMML</code> library expects a map as input.\nDon't get intimidated by the code below, it's just converting the <code>Erlang</code> map to a <code>C++</code> map by iterating over all the keys and value pairs.</p>\n<p>For prediction, we will need a map with the key as <code>X</code> and value as the input.</p>\n<blockquote>\n<p>I could have just used a binary input as X and converted it to a string on the <code>C/C++</code> side. But, to make this more general, I am converting the <code>Erlang</code> map to a <code>C++</code> map. Now this can be used for any PMML file and not just a specific one.</p>\n</blockquote>\n<pre><code>static ERL_NIF_TERM predict(ErlNifEnv* env, int argc, const ERL_NIF_TERM argv[]) {\n  unordered_map&lt;std::string, std::string&gt; cpp_map;\n\n  if (!enif_is_map(env, argv[0]))\n  {\n    return enif_make_badarg(env);\n  }\n\n  ErlNifMapIterator iter;\n  if (enif_map_iterator_create(env, argv[0], &amp;iter, ERL_NIF_MAP_ITERATOR_FIRST))\n  {\n    do\n    {\n      ERL_NIF_TERM key, value;\n      if (enif_map_iterator_get_pair(env, &amp;iter, &amp;key, &amp;value))\n      {\n        ErlNifBinary key_bin, value_bin;\n        if (enif_map_iterator_get_pair(env, &amp;iter, &amp;key, &amp;value))\n        {\n          char key_str[64], value_str[64];\n          if (enif_get_string(env, key, key_str, sizeof(key_str), ERL_NIF_LATIN1) &amp;&amp;\n              enif_get_string(env, value, value_str, sizeof(value_str), ERL_NIF_LATIN1))\n          {\n            cpp_map[key_str] = value_str;\n          }\n        }\n      }\n    } while (enif_map_iterator_next(env, &amp;iter));\n    enif_map_iterator_destroy(env, &amp;iter);\n  }\n\n  string ret = pmmlModelParser-&gt;predict(cpp_map);\n  return enif_make_string(env, ret.c_str(), ERL_NIF_LATIN1);\n}\n</code></pre>\n<h3>Erlang code</h3>\n<p>We will name our <code>Erlang</code> module <code>lr_model.erl</code>.</p>\n<pre><code>-module(lr_model).\n-export([init/1, evaluate/1]).\n-nifs([init/1, evaluate/1]).\n-on_load(init/0).\n\ninit() -&gt;\n    ok = erlang:load_nif(\"./lr_model\", 0).\n\ninit(PmmlFile) -&gt;\n    exit(problem_loading_nif).\n\nevaluate(Input) -&gt;\n    exit(problem_loading_nif).\n</code></pre>\n<h3>Compiling and Running</h3>\n<p>Compiling the <code>C/C++</code> code will now take an extra parameter to include <code>cPMML</code> library.\nAs the model is predicting the values for <code>y = 2x + 1</code>, we should get <code>~3</code> for <code>x = 1</code> and <code>~1</code> for <code>x = 0</code>.</p>\n<pre><code>g++ -std=c++11 library.cpp \\\n-o lr_model.so \\\n-lcPMML \\\n-I /usr/local/lib/erlang/erts-13.2.2.2/include/ \\\n-bundle \\\n-bundle_loader /usr/local/lib/erlang/erts-13.2.2.2/bin/beam.smp\n</code></pre>\n<pre><code>&gt; c(lr_model).\n&gt; lr_model:init(&lt;&lt;\"./lr_model.pmml\"&gt;&gt;).\n&gt; lr_model:evaluate(#{\"X\"=&gt;\"0\"}).\n\"0.967265\"\n&gt; lr_model:evaluate(#{\"X\"=&gt;\"1\"}).\n\"3.007469\"\n</code></pre>\n<h2>Conclusion</h2>\n<p>In this blog, we saw how to use <code>C/C++</code> libraries in <code>Erlang</code> code.\nWe used a PMML file of a linear regression model to predict the values of <code>y = 2x + 1</code> for a given value of <code>x</code> in <code>Erlang</code> where the code was implemented in C++.\nWe saw how to use the <code>cPMML</code> library to parse the <code>PMML</code> file and use it for predictions.</p>\n",
            "enclosure": {},
            "categories": []
        },
        {
            "title": "Running AI/ML predictions in CPP using cPMML library",
            "pubDate": "2023-10-01 23:39:47",
            "link": "https://blog-schwiftycold.firebaseapp.com/blog/2023-10-02-cpmml-predictions/",
            "guid": "https://blog-schwiftycold.firebaseapp.com/blog/2023-10-02-cpmml-predictions/",
            "author": "",
            "thumbnail": "",
            "description": "This is a short blog on how to install cPMML library and run AI/ML predictions in CPP.",
            "content": "\n<h2>Introduction</h2>\n<p><code>PMML</code> is a markup language to save your AI/ML model files\nso that you can use them for predictions later on\n(maybe during production).\n<code>cPMML</code> is a library created by the\n<a href=\"https://github.com/AmadeusITGroup/cPMML\">AmadeusITGroup</a>\nto parse and run predictions in C++.\nIn this blog, we will train a linear regression model in\n<code>python</code> and generate a <code>pmml</code> file and then we will\nrun our predictions in <code>C++</code>.</p>\n<h2>Creating a model file</h2>\n<h3>Dependencies</h3>\n<p>We will need <code>pandas</code>, <code>numpy</code>, <code>scikit-learn</code> and <code>sklearn2pmml</code>.</p>\n<pre><code>pip install pandas numpy scikit-learn sklearn2pmml\n</code></pre>\n<h4>Imports</h4>\n<pre><code>import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn2pmml import sklearn2pmml\nfrom sklearn2pmml.pipeline import PMMLPipeline\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\n</code></pre>\n<h3>The model</h3>\n<h4>Dataset</h4>\n<p>For keeping things simple, let's train a linear regression model\nto match the equation, <code>y = 2x + 1</code>.\nWe can generate a random dataset for this equation.</p>\n<pre><code>X = np.random.rand(100, 1)\nY = 2 * X + 1 + 0.1 * np.random.randn(100, 1)\n</code></pre>\n<h4>Test/Train data</h4>\n<p>Next, we'll divide the data into test and train datasets.</p>\n<pre><code>df = pd.DataFrame({'X': X.flatten(), 'Y': Y.flatten()})\ntrain_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\nX_train = train_df[['X']]\ny_train = train_df['Y']\nX_test = test_df[['X']]\ny_test = test_df['Y']\n</code></pre>\n<h4>Training the model</h4>\n<p>For training the model, we can get the model from scikit\nlearn library and use the dataset we generated above.\nWe can also check the <code>mse</code> to get an idea of the model's accuracy.</p>\n<pre><code>pipeline = PMMLPipeline([\n    (\"regressor\", LinearRegression())\n])\n\npipeline.fit(X_train, y_train)\n\ny_pred = pipeline.predict(X_test)\nmse = mean_squared_error(y_test, y_pred)\nprint(f\"Mean Squared Error: {mse}\")\n</code></pre>\n<h4>Saving the pmml file</h4>\n<p>If you are satisfied by the performance of your model, you can\nexport the model as a pmml file.\nWe will save the model with the name, <code>lr_model.pmml</code></p>\n<pre><code>sklearn2pmml(pipeline, \"lr_model.pmml\", with_repr = True)\n</code></pre>\n<h2>Using the model file</h2>\n<p>The main step of focus in this blog is using the model in C++ program.\nFor this, you will need to isntall the <code>cPMML</code> library.</p>\n<h3>Installing cPMML</h3>\n<p>To install the libray in your system, you just need to run the below command.\nThis will run <code>cmake</code>, so you should have <code>cmake</code> installed in your system.</p>\n<pre><code>git clone https://github.com/AmadeusITGroup/cPMML.git &amp;&amp; cd cPMML &amp;&amp; ./install.sh\n</code></pre>\n<h4>For Mac M1</h4>\n<p>I ran into some problems while installing this on Mac M1.\nHere are the steps to install this effortlessly.</p>\n<ol>\n<li>Ensure you have the latest version of <code>cmake</code> installed in your system.</li>\n<li>You can edit the <code>install.sh</code> script to remove <code>-j 4</code> flag from the <code>cmake -j 4 ..</code> command. This will turn off the multi processing.</li>\n<li>The last line of the <code>install.sh</code> script is <code>sudo ldconfig</code>. Change this to <code>sudo update_dyld_shared_cache</code>. This installs the <code>.dylib</code> or <code>.so</code> library files to proper destination.</li>\n</ol>\n<h3>Running the predictions</h3>\n<h4>Include the library</h4>\n<p>The first thing is to import the library.</p>\n<pre><code>#include \"cPMML.h\"\n#include &lt;iostream&gt;\n</code></pre>\n<h4>Load the model</h4>\n<p>Then you can load the model.</p>\n<pre><code>int main() {\n  cpmml::Model model(\"lr_model.pmml\");\n  return 0;\n}\n</code></pre>\n<h4>Start predictions</h4>\n<p>The <code>cPMML</code> library takes input as an unordered_map of strings.\nFor us, there is only one input which is <code>X</code>.</p>\n<pre><code>int main() {\n  cpmml::Model model(\"lr_model.pmml\");\n\n  // This shoule yield a value close to 1\n  std::unordered_map&lt;std::string, std::string&gt; input1 = {\n    {\"X\", \"0\"}\n  };\n\n\n  // This should yield a value close to 21\n  std::unordered_map&lt;std::string, std::string&gt; input2 = {\n    {\"X\", \"10\"}\n  };\n\n  std::cout&lt;&lt;\"X = 0 Y = \"&lt;&lt;model.predict(input1)&lt;&lt;'\\n';\n  std::cout&lt;&lt;\"X = 10 Y = \"&lt;&lt;model.predict(input2)&lt;&lt;'\\n';\n  \n  return 0;\n}\n</code></pre>\n<h4>Compilation</h4>\n<p>You can compile the code by including the <code>cPMML</code> library.</p>\n<pre><code>&gt; g++ -std=c++11 predict.cpp -o predict.o -lcPMML\n&gt; ./predict.o\nX = 0 Y = 0.967265\nX = 10 Y = 21.369305\n</code></pre>\n<h2>Conclusion</h2>\n<p>In this blog, we saw how to store your model as a <code>PMML</code> file and load it in <code>C++</code> using <code>cPMML</code> library.\nYou can view the code for the above <a href=\"https://github.com/UnresolvedCold/prediction-pmml-using-cpmml-in-cpp.git\">here</a>.</p>\n",
            "enclosure": {},
            "categories": []
        },
        {
            "title": "Embedded Jetty server with handlers for legacy Java applications",
            "pubDate": "2023-09-01 18:30:00",
            "link": "https://blog-schwiftycold.firebaseapp.com/blog/2023-09-02-embedded-jetty-handlers/",
            "guid": "https://blog-schwiftycold.firebaseapp.com/blog/2023-09-02-embedded-jetty-handlers/",
            "author": "",
            "thumbnail": "",
            "description": "A simple way to embed a Jetty server in a Java application and add handlers for different requests",
            "content": "\n<p>import PlantUML from '../../components/PlantUML.astro'</p>\n<h2>Introduction</h2>\n<p><a href=\"https://eclipse.dev/jetty/\">Jetty</a> is a very powerful yet lightweight <code>Java</code> library since ages that helps you create servers and clients for HTTP (literally all the versions), Web Sockets, OSGI, JMX, JAAS and much more.</p>\n<p>This post will deal with embedding a Jetty server in a Java application and adding handlers for different requests.\nI'm assuming you have a basic understanding of Java and Maven and have familiarity with different HTTP methods like GET, POST, PUT, DELETE, etc.</p>\n<h2>Jetty Server Architecture</h2>\n<p>An incoming request is handled using 4 components - Threadpool, Connectors, Handlers and Server.\n<code>Server</code> is the core of the system that handles the management of servers and the entire lifecycle of the server.\n<code>Connectors</code> help in accepting various requests over different protocols like HTTP, HTTPS, etc.\n<code>Handlers</code> are the components that process the incoming request.\n<code>Threadpools</code> are like tiny workers that make the system multi-threaded and help in handling multiple requests at the same time.</p>\n<p>An incoming request is first accepted at the <code>Connector</code>.\nThen it is sent to the <code>Server</code> which connects it to the <code>Handler</code> where the main response is generated.\nAnd the response is generated using a particular thread as per <code>Threadpool</code>.</p>\n<p>&lt;!plantuml/&gt;</p>\n<h2>Using Jetty in a Java application</h2>\n<h3>New Java App</h3>\n<p>First, let's create a new <code>Java</code> app using <code>Maven</code>.</p>\n<pre><code>&gt; mvn archetype:generate \\\n-DgroupId = com.schwiftycold.poc \\\n-DartifactId = poc_jetty_server \\\n-DarchetypeArtifactId = maven-archetype-quickstart \\\n-DinteractiveMode = false\n</code></pre>\n<p>Add the following properties to your <code>POM</code> file.\nThis will set the <code>Java</code> version and language encoding for the app.</p>\n<pre><code>  &lt;properties&gt;\n    &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;\n    &lt;maven.compiler.source&gt;20&lt;/maven.compiler.source&gt;\n    &lt;maven.compiler.target&gt;20&lt;/maven.compiler.target&gt;\n  &lt;/properties&gt;\n</code></pre>\n<h3>Add Jetty dependency</h3>\n<p>Add the following dependency in your <code>pom.xml</code> file.</p>\n<pre><code>&lt;dependencies&gt;\n  &lt;dependency&gt;\n    &lt;groupId&gt;org.eclipse.jetty&lt;/groupId&gt;\n    &lt;artifactId&gt;jetty-server&lt;/artifactId&gt;\n    &lt;version&gt;11.0.3&lt;/version&gt;\n  &lt;/dependency&gt;\n  ...\n&lt;/dependencies&gt;\n</code></pre>\n<h3>Main server</h3>\n<p>The main component of the system is the server class.\nThis class will create a server context and register connectors and handlers.\nLet's call this class <code>MainServer</code>.</p>\n<h4>Threadpool</h4>\n<p>There are different kinds of Threadpool offered by Jetty like <code>QueuedThreadPool</code>, <code>ExecutorThreadPool</code>, <code>ScheduledThreadPool</code>, etc.</p>\n<p><code>QueuedThreadPool</code> maintains a fixed number of threads and a queue to manage incoming requests.\nWhen a request comes and the thread is busy then it is added to the queue and will be picked when a thread is available.\nAnd, due to its nature, this is widely used for handling <code>http</code> requests.</p>\n<p><code>ScheduledThreadPool</code> extends <code>QueuedThreadPool</code> to handle scheduled tasks.\nIt provides us with a scheduler to execute tasks at a certain interval.\nIt is generally used for scheduling background tasks.</p>\n<p><code>ExecutorThreadPool</code> enables you to use custom <code>Executor</code> as the Threadpool for Jetty.</p>\n<p>Here, we will be using <code>QueuedThreadPool</code>.</p>\n<pre><code>ThreadPool threadPool = new QueuedThreadPool();\n</code></pre>\n<h4>Server</h4>\n<p>The server is the essential component that manages the connectors and handlers.\nIt takes the Threadpool to create a new server instance.</p>\n<pre><code>Server server = new Server(threadPool);\n</code></pre>\n<h4>Connector</h4>\n<p>A connector allows us to accept a variety of different protocols like <code>HTTP</code>, <code>HTTPS</code>, <code>Unix domain socket</code>, etc.</p>\n<h5>HTTP</h5>\n<p>A simple <code>HTTP</code> connector can be initialized using the <code>ServerConnector</code> class.\nAnd you can change the port to listen to using, <code>setPort</code> function.\nBy default, the port is <code>8080</code>.</p>\n<pre><code>ServerConnector connector = new ServerConnector(server);\nconnector.setPort(9120);\nserver.setConnectors(new Connector[]{connector});\n\n</code></pre>\n<h5>HTTPS</h5>\n<p>For using <code>HTTPS</code> you'll need to register the <code>SSL/TLS</code> certificate.\nWe won't be using this here but a quick look into this will give us a glance of the infinite possibilities.</p>\n<p>First, you will need the sslContextFactory which defines your <code>SSL/TLS</code> configurations.</p>\n<pre><code>SslContextFactory sslContextFactory = new SslContextFactory();\nsslContextFactory.setKeyStorePath(\"/path/to/keystore.jks\");\nsslContextFactory.setKeyStorePassword(\"keystore-password\");\nsslContextFactory.setKeyManagerPassword(\"key-password\");\n</code></pre>\n<pre><code>ServerConnector httpsConnector = new ServerConnector(\n    server,\n    new SslConnectionFactory(sslContextFactory, \"http/1.1\")\n);\nhttpsConnector.setPort(8443);\nserver.setConnectors(new Connector[] {httpsConnector});\n</code></pre>\n<h4>Handlers</h4>\n<p>Handlers are the ones that will be creating or response.\nFor this, let's create a new singleton class.\nLet's set this to <code>null</code> for now.</p>\n<pre><code>server.setHandler(null);\n</code></pre>\n<h4>Final code</h4>\n<pre><code>public class MainServer {\n  public static void startServer()  {\n    try {\n      ThreadPool threadPool = new QueuedThreadPool();\n\n      Server server = new Server(threadPool);\n\n      ServerConnector connector = new ServerConnector(server);\n      connector.setPort(9120);\n      \n      server.setConnectors(new Connector[]{connector});\n      server.setHandler(null);\n\n      server.start();\n      server.join();\n    } catch (Exception e) {\n      e.printStackTrace();\n    }\n  }\n}\n\n</code></pre>\n<h3>Handler class</h3>\n<p>Handler class is where we will write our logic to process the request.\nSo this requires a special attention.</p>\n<p>Let's create a new class that extends <code>AbstractHandler</code></p>\n<h4>Extend AbstractHandler</h4>\n<p>The <code>AbstractHandler</code> required us to implement a <code>handle</code> function.\nThis function is triggered on every new request.</p>\n<p>The handle method uses four parameters.\nThe <code>target</code> parameter denotes the endpoint that was triggered.\nIf we trigger the URL, <code>http://localhost:9120/hi</code> then the target will take the value as <code>/hi</code>.</p>\n<p><code>baseRequest</code> and <code>request</code> denote the same thing but in a different context.\n<code>baseRequest</code> is Jetty specific whereas <code>request</code> donates the servlet-specific APIs.\nHere, we won't be using the servlet APIs.</p>\n<p>The <code>response</code> is the processed result generated by the server which will be sent to the client.</p>\n<pre><code>public class MainHandler extends AbstractHandler{\n\n  @Override\n  public void handle(String target, Request baseRequest, HttpServletRequest request, HttpServletResponse response)\n      throws IOException, ServletException {\n        System.out.println(\"target: \" + target);\n        System.out.println(\"baseRequest: \" + baseRequest);\n        System.out.println(\"request: \" + request);\n        System.out.println(\"response: \" + response);     \n  }\n}\n</code></pre>\n<p>The output of the above code would be something as follows on triggering with <code>curl</code>.</p>\n<pre><code>&gt; curl -X POST \"http://localhost:9120/hi\" -d \"p=value\"\n\ntarget: /hi\nbaseRequest: Request(POST http://localhost:9120/hi)@57ffc3ca\nrequest: Request(POST http://localhost:9120/hi)@57ffc3ca\nresponse: HTTP/1.1 200\n</code></pre>\n<p>Here, you can see the target as <code>/hi</code>, the request denotes the method call and reference to the request object and the response is just 200 which denotes a success.\nWe can manipulate this information to create responses based on different inputs.</p>\n<h4>Manage different methods</h4>\n<p>We can get the request method using the response object by calling the function, <code>getMethod</code>.</p>\n<pre><code>public void handle(...) {\n    if (\"POST\".equalsIgnoreCase(baseRequest.getMethod())) {\n      System.out.println(\"POST request received\");\n    }\n    else if (\"GET\".equalsIgnoreCase(baseRequest.getMethod())) {\n      System.out.println(\"GET request received\");\n    }\n}\n</code></pre>\n<p>Now, calling curl as above will print <code>POST request received</code> on the server side.</p>\n<h4>Extract the query parameters</h4>\n<p>The query parameter and its values can be extracted from the base request object by using the <code>getParameterNames</code> method.\nHere, I'm parsing the values and storing them in a <code>Map</code> for accessing them later.</p>\n<pre><code>if (\"POST\".equalsIgnoreCase(baseRequest.getMethod())) {\n  System.out.println(\"POST request received\");\n  Map&lt;String, String&gt; queryParams = new HashMap&lt;&gt;();\n\n  for (Enumeration&lt;String&gt; e = request.getParameterNames(); e.hasMoreElements();) {\n    String name = e.nextElement();\n    String[] values = request.getParameterValues(name);\n    for (String value : values) {\n      queryParams.put(name, value);\n    }\n  }\n\n  System.out.println(\"Query params: \" + queryParams);\n  ...\n</code></pre>\n<p>Now, you can see the below response on the server side.</p>\n<pre><code>POST request received\nQuery params: {p=value}\n</code></pre>\n<h4>Generating the result</h4>\n<p>The last part is generating the result for specific triggers.\nThis is done by using <code>getWriter</code> function to write to the response.\nYou can also set the status of the response using <code>setStatus</code> method.\nYou can create more <code>if</code> statements for each endpoint.</p>\n<pre><code>try {\n  if (target.startsWith(\"/hi\")) {\n    response.setStatus(HttpServletResponse.SC_OK);\n    response.getWriter().println(\"Hello World\");\n    baseRequest.setHandled(true);\n  }\n} catch (Exception e) {\n  e.printStackTrace();\n}\n</code></pre>\n<h4>Final handle method</h4>\n<pre><code>  public void handle(String target, Request baseRequest, HttpServletRequest request, HttpServletResponse response)\n      throws IOException, ServletException {\n    if (\"POST\".equalsIgnoreCase(baseRequest.getMethod())) {\n      System.out.println(\"POST request received\");\n      Map&lt;String, String&gt; queryParams = new HashMap&lt;&gt;();\n\n      for (Enumeration&lt;String&gt; e = request.getParameterNames(); e.hasMoreElements();) {\n        String name = e.nextElement();\n        String[] values = request.getParameterValues(name);\n        for (String value : values) {\n          queryParams.put(name, value);\n        }\n      }\n\n      System.out.println(\"Query params: \" + queryParams);\n\n      System.out.println(\"Target: \" +target.startsWith(\"/hi\"));\n\n      try {\n        if (target.startsWith(\"/hi\")) {\n          System.out.println(\"Target: \" +target);\n          response.setStatus(HttpServletResponse.SC_OK);\n          response.getWriter().println(\"Hello World\");\n          baseRequest.setHandled(true);\n        }\n      }\n      catch (Exception e) {\n        e.printStackTrace();\n      }\n\n    } else if (\"GET\".equalsIgnoreCase(baseRequest.getMethod())) {\n      System.out.println(\"GET request received\");\n    }\n  }\n\n</code></pre>\n<h4>Connect Handler and  MainServer</h4>\n<p>Before testing your new server, you'll also need to add the handler which can be done using the server's <code>setHandler</code> method.</p>\n<pre><code>server.setHandler(new MainHandler());\n</code></pre>\n<p>Now calling the <code>hi</code> endpoint will return <code>Hello World</code> as a response.</p>\n<pre><code>&gt; curl -X POST \"http://localhost:9120/hi\" -d \"p=value\"\n\nHello World\n</code></pre>\n<h2>Conclusion</h2>\n<p>In this post, we saw how to configure <code>Jetty</code> server and set handlers to generate responses for a particular endpoint trigger.\nThis looks like a long post but it revolves around just creating a 'Hello World' server using <code>Jetty</code>.\nYou can find the code for the above <a href=\"https://github.com/UnresolvedCold/POC-Jetty-Server-Handlers\">here</a>.</p>\n",
            "enclosure": {},
            "categories": []
        },
        {
            "title": "Communication b/w Java (Maven) and Erlang (rebar3) using Jinterface",
            "pubDate": "2023-08-26 09:38:06",
            "link": "https://blog-schwiftycold.firebaseapp.com/blog/2023-08-22-jinterface-erlang/",
            "guid": "https://blog-schwiftycold.firebaseapp.com/blog/2023-08-22-jinterface-erlang/",
            "author": "",
            "thumbnail": "",
            "description": "Jinterface is a way to make Java programs behave like Erlang nodes. This ensures seamless communication between Erlang and Java programs.",
            "content": "\n<h2>Introduction</h2>\n<p><code>Jinterface</code> is a way to make <code>Java</code> programs behave like <code>Erlang</code> nodes.\nThis ensures seamless communication between <code>Erlang</code> and <code>Java</code> programs.\n<code>Jinterface</code> exposes some APIs that you can use to create <code>Erlang</code> data structures and send them to <code>Erlang</code> nodes.</p>\n<h2>How to use Jinterface</h2>\n<h3>Installation</h3>\n<p>For using <code>Jinterface</code> you need a <code>jar</code> file provided by <code>Erlang</code> called <code>OtpErlang.jar</code>.\nYou can find the location of the <code>jar</code> by opening <code>erlang</code> shell and typing the below command.\nThis will provide you with the location of the jar file.</p>\n<pre><code>&gt; code:priv_dir(jinterface).\n\"/usr/local/lib/erlang/lib/jinterface-1.13.2/priv\"\n</code></pre>\n<pre><code>$ ls \"/usr/local/lib/erlang/lib/jinterface-1.13.2/priv\"\nOtpErlang.jar\n</code></pre>\n<p>It may happen that the file isn't there. This is because <code>Jinterface</code> is not installed by default.\nIn this case, you will need to install <code>Erlang</code> from the source with <code>Java 8+</code> installed on the path.</p>\n<blockquote>\n<p>If you have installed <code>Erlang</code> using <code>homebrew</code> on <em>macOS</em> then it might not be there.</p>\n</blockquote>\n<p>So first make sure, you have <code>Java</code> runtime installed. If not, then please install this first.</p>\n<pre><code>$ java -version\nopenjdk version \"20.0.1\" 2023-04-18\nOpenJDK Runtime Environment (build 20.0.1+9-29)\nOpenJDK 64-Bit Server VM (build 20.0.1+9-29, mixed mode, sharing)\n</code></pre>\n<p>Remove your old installation of <code>Erlang</code> if you have any.\nThen clone the <code>Erlang</code> repository and build it from the source.</p>\n<pre><code>$ git clone https://github.com/erlang/otp.git\n$ cd otp\n$ git checkout maint-25\n$ ./configure &amp;&amp; make &amp;&amp; make install\n</code></pre>\n<p>Also, verify if the jar has been installed correctly as we did earlier.</p>\n<h3>Creating a Java node</h3>\n<h4>New Maven Project</h4>\n<p>Let's first create a new <code>maven</code> project.</p>\n<pre><code>$ mvn archetype:generate -DgroupId=com.example \\\n    -DartifactId=myproject \\\n    -DarchetypeArtifactId=maven-archetype-quickstart \\\n    -DinteractiveMode=false\n</code></pre>\n<p>And you might also want to update the <code>Java</code> version and <code>encoding</code> by adding/updating the below properties in <code>pom.xml</code>.</p>\n<pre><code>&lt;properties&gt;\n    &lt;maven.compiler.source&gt;20&lt;/maven.compiler.source&gt;\n    &lt;maven.compiler.target&gt;20&lt;/maven.compiler.target&gt;\n        &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;\n    &lt;project.reporting.outputEncoding&gt;UTF-8&lt;/project.reporting.outputEncoding&gt;\n&lt;/properties&gt;\n</code></pre>\n<h4>Adding Jinterface dependency</h4>\n<p>The easiest way is to install your <code>jar</code> in <code>m2</code> and add it to your dependency.</p>\n<pre><code>$ mvn install:install-file \\\n   -Dfile=/usr/local/lib/erlang/lib/jinterface-1.13.2/priv/OtpErlang.jar \\\n   -DgroupId=com.ericsson.otp \\\n   -DartifactId=erlang \\\n   -Dversion=1.13.2 \\\n   -Dpackaging=jar \\\n   -DgeneratePom=true\n</code></pre>\n<p>And then add the dependency in <code>pom.xml</code>.</p>\n<pre><code>&lt;dependency&gt;\n    &lt;groupId&gt;com.ericsson.otp&lt;/groupId&gt;\n    &lt;artifactId&gt;erlang&lt;/artifactId&gt;\n    &lt;version&gt;1.13.2&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre>\n<h4>Creating a Java node</h4>\n<p>This is the fun part.\nEach <code>Java</code> app will contain a node and a mailbox.\nWe can send the message to the node on a particular mailbox and the node will receive the message and process it exactly like <code>Erlang</code> nodes.</p>\n<p>But first, import the required packages.</p>\n<pre><code>import com.ericsson.otp.erlang.*;\n</code></pre>\n<p>Let's create a node called, 'java_node' and a mailbox called, 'java_mailbox'.</p>\n<pre><code>OtpNode node = new OtpNode(\"java_node\");\nOtpMbox mbox = node.createMbox(\"java_mailbox\");\n</code></pre>\n<p>Next, let's say we will be sending the node a tuple containing the <code>Pid</code> of the Erlang node and a message atom called <code>hello</code>.\nOn receiving <code>{Pid, hello}</code> as the message, we will send 'world' as a message to the calling node.</p>\n<p>The <code>mbox.receive()</code> function will pause the execution of the program until it receives a message.\nWe are typecasting the received message to <code>OtpErlangTuple</code> because we know that the message will be a tuple and let's extract the <code>Pid</code> and the message atom from the tuple.</p>\n<pre><code>// Get the message\nOtpErlangTuple erlTuple = (OtpErlangTuple) mbox.receive();\n\n// Parse the message\nOtpErlangPid fromPid = (OtpErlangPid) erlTuple.elementAt(0);\nOtpErlangAtom atom = (OtpErlangAtom) erlTuple.elementAt(1);\n</code></pre>\n<p>Now we will check if the message is <code>hello</code> and if that is the case, we will send <code>world</code> as a message to the calling node.\nMessages can be sent using the <code>mbox.send()</code> function.</p>\n<pre><code>if (atom.atomValue().equals(\"hello\")) {\n    // Create the reply message\n    OtpErlangAtom replyAtom = new OtpErlangAtom(\"world\");\n    OtpErlangObject[] replyElements = {new OtpErlangAtom(\"ok\"), replyAtom};\n    OtpErlangTuple replyTuple = new OtpErlangTuple(replyElements);\n    \n    // This will send the message to the calling node with Pid as fromPid\n    mbox.send(fromPid, replyTuple);\n}\n</code></pre>\n<h4>Alive function</h4>\n<p>You might want to create a health checker function to get a reply on pinging the node.\nThis is done by creating a <code>isAlive</code> function and it comes in handy while debugging and in code as a sanity check.</p>\n<pre><code>public boolean isAlive() {\n        return true;\n    }\n</code></pre>\n<h4>Final Java code</h4>\n<p>Your final <code>Java</code> code will look something like this.</p>\n<pre><code>package com.example;\n\nimport com.ericsson.otp.erlang.*;\n\npublic class HelloWorld {\n    public boolean isAlive() {\n            return true;\n        }\n\n    public static void main(String[] args) throws Exception {\n         OtpNode node = new OtpNode(\"java_node\");\n         OtpMbox mbox = node.createMbox(\"java_mailbox\");\n         System.out.println(\"Node Created. Now, you can communicate with this node.\");\n         OtpErlangTuple erlTuple = (OtpErlangTuple) mbox.receive();\n         OtpErlangPid fromPid = (OtpErlangPid) erlTuple.elementAt(0);\n         OtpErlangAtom atom = (OtpErlangAtom) erlTuple.elementAt(1);\n         if (atom.atomValue().equals(\"hello\")) {\n             OtpErlangAtom replyAtom = new OtpErlangAtom(\"world\");\n             OtpErlangObject[] replyElements = {new OtpErlangAtom(\"ok\"), replyAtom};\n             OtpErlangTuple replyTuple = new OtpErlangTuple(replyElements);\n             mbox.send(fromPid, replyTuple);\n         }\n    }\n}\n\n</code></pre>\n<p>You can verify if your <code>Java</code> app is being registered as a node using the below commands in the <code>Erlang</code> shell.\nAs we have implemented the <code>isAlive</code> function, we can also <code>ping</code> the node.\nA return value of <code>pong</code> means success and <code>pang</code> means failure.</p>\n<pre><code>$ erl -sname client\n&gt; net_adm:names().\n{ok,[{\"java_node\",59873}]}\n\n&gt; net_adm:ping(java_node@GGN002262).       \npong\n</code></pre>\n<h3>Erlang Node</h3>\n<h4>Erlang program</h4>\n<p>On the <code>Erlang</code> side we can just send the message to the <code>java_node</code> and wait for a message.</p>\n<blockquote>\n<p>The <code>GGN002262</code> part of my node is my hostname. Use the <code>hostname</code> command to get yours.</p>\n</blockquote>\n<pre><code>-module(client).\n-export([start/0]).\n\nstart() -&gt;\n  {java_mailbox, 'java_node@GGN002262'} ! {self(), hello},\n  receive\n\n  {ok, Res} -&gt;\n     io:format(\"Java says: ~p~n\", [Res])\n  end.\n</code></pre>\n<h4>Run a distributed service</h4>\n<p>You will need to start the <code>Erlang</code> runtime as a distributed service.\nThis can be done using <code>-sname</code> flag.</p>\n<pre><code>$ erl -sname client\n1&gt; c(client).\n2&gt; client:start().\nJava says: world\n</code></pre>\n<h2>Starting EPMD</h2>\n<p><code>epmd</code> program comes prepacked with the <code>Erlang</code> distribution.\nThis is the <code>Erlang</code> port manager.\nThis is required for registering nodes.</p>\n<p>If <code>epmd</code> is not running then you will get an error something like this, <code>Nameserver not responding on GGN002262 when publishing java_node</code>.</p>\n<p>To start the <code>epmd</code> server, you just need to run <code>epmd</code> from your shell.</p>\n<pre><code># Start epmd server in the background\n$ epmd&amp;\n\n# Run the Java app\n&gt; java -jar my-app.jar\n\n# Verify if the app is getting registered.\n$ epmd -names\nepmd: up and running on port 4369 with data:\nname java_node at port 59873\n</code></pre>\n<p>If your <code>Java</code> app is registered, you will get the above output.</p>\n<h2>Conclusion</h2>\n<p>Here, we discussed how to use <code>Jinterface</code> to communicate with <code>Erlang</code> using <code>Java</code>.\nWe saw that <code>Java</code> programs can behave as <code>Erlang</code> nodes and can communicate with each other through message passing.\nWe also learned about <code>epmd</code>, the Erlang port manager.</p>\n<p>One thing to consider while using <code>Jinterface</code> is it simulates the <code>Java</code> node as an <code>Erlang</code> node, which makes the communication a little bit slow hence this approach should not be taken for a system that requires high-frequency message transfer.</p>\n<p>It is also important to note that by default, this process of message transfer is <code>async</code> which means we will need to wait for <code>Java</code> to process the result and send a message back.\nThere are ways to make this <code>synchronous</code> by creating a mechanism to send a confirmation message from the client side on receiving the processed message.</p>\n<p>Overall, this is a great add-on to <code>Erlang</code> that pumps the power of <code>Java</code> into <code>Erlang's</code> world.</p>\n",
            "enclosure": {},
            "categories": []
        },
        {
            "title": "Docker Desktop vs Colima on Mac M1 for working with VSCode containers",
            "pubDate": "2023-08-18 16:01:22",
            "link": "https://blog-schwiftycold.firebaseapp.com/blog/2023-08-18-analysis-docker-dsktop-colima-on-mac-m1/",
            "guid": "https://blog-schwiftycold.firebaseapp.com/blog/2023-08-18-analysis-docker-dsktop-colima-on-mac-m1/",
            "author": "",
            "thumbnail": "",
            "description": "A read/write speed comparison between Docker Desktop and Colima on Mac M1 for developing in devcontainers and VSCode.",
            "content": "\n<h2>Introduction</h2>\n<p>I am assuming that you have a basic understanding of <code>docker</code> and <code>devcontainers</code> and have used them before.\nAs this post revolves around Mac M1, I'd suggest not using this analysis for comparing other systems in general.</p>\n<p>VSCode <code>devcontainers</code> are the new way of starting a project development environment.\nThey are a great way to get started with a project without having to install all the dependencies on your local machine.\nThis reduces the setup time and allows you to get started with the project right away.</p>\n<p><code>Devcontainers</code> are powered by Docker and VSCode.\nVSCode provides the UI and Docker provides the containerization.\nThis means that you need to have Docker installed on your machine to use <code>devcontainers</code>.</p>\n<p>While <code>docker</code> is the main engine,\n<code>Docker Desktop</code> and <code>Colima</code> are the two main options for creating a <code>docker</code> environment on Mac.</p>\n<p><code>Colima</code> is an open-source alternative to <code>Docker Desktop</code>.\nWhile both software are free to use, <code>Docker Desktop</code> requires companies to pay for a license if they have more than 250 employees.\nAnd also, <code>Docker Desktop</code> can be enhanced with lots of extensions which Colima can't.</p>\n<p>I am using <code>docker</code> for mainly 2 things, creating production-level containers and using <code>devcontainers</code> for development.\nSo a lot of <code>Docker Desktop</code> extensions are not useful for me but the performance is.</p>\n<p>I analyzed the read/write performance of <code>Docker Desktop</code> and <code>Colima</code> for working with <code>devcontainers</code> and here are the results.\nI also compared the build time for my blog on both the software.</p>\n<h2>System Information</h2>\n<p>I am using a Macbook Air M1 with 16 GB RAM.\nI have allocated 8 GB RAM and 4 CPUs to Docker Desktop and Colima each.\nI also kept the disk storage to 60 GB for both the software which would be enough for our testing.</p>\n<h3>Docker Desktop</h3>\n<p>I'm using Docker Desktop <code>v24.2</code> which comes with a <code>60%</code> improvement in read/write performance (as they say).\nIt's using Apple's <code>VZ</code> for virtualization and the net says it is far more optimized than <code>Qemu</code>.\nI have no extensions installed on Docker Desktop.\nAnd no other container was running at the time of analysis.</p>\n<h3>Colima</h3>\n<p>The latest version of Colima is <code>v0.5.5</code> at the time of writing this article.\nIt was released on May 2023.\nFor the comparison, I'm using the <code>HEAD</code> of the <a href=\"https://github.com/abiosoft/colima\">Colima</a> repository to capture any latest improvements.</p>\n<p>I tested Colima with both <code>Qemu</code> and <code>VZ</code> virtualization.</p>\n<blockquote>\n<p>Using the <code>HEAD</code> can be unstable compared to the release branches.</p>\n</blockquote>\n<h2>Analysis Procedure</h2>\n<p>I created a simple <code>Java</code> app for writing files of size <code>1 MB</code> to <code>1 GB</code> and calculated the time it takes to write each of the files.\nFile sizes are <code>1 MB</code>, <code>10 MB</code>, <code>64 MB</code>, <code>128 MB</code>, <code>256 MB</code>, <code>512 MB</code>, and <code>1024 MB</code>.</p>\n<p>I created an arbitrary <code>1 GB</code> file and accessed the random positions <code>1,000</code> to <code>1,000,000</code> times and calculated the average.</p>\n<p>I also calculated the time it takes to print lines on the console using <code>sout</code>.\nI printed <code>10,000</code>, <code>100,000</code>, and <code>1,000,000</code> lines of <code>1</code> to <code>1000</code> characters each and calculated the average time.</p>\n<blockquote>\n<p>You can find the Java app <a href=\"https://github.com/UnresolvedCold/poc-performance-devcontainer\">here</a>.</p>\n</blockquote>\n<p>My blog is made on <code>AstroJs</code> which generates a static site on build.\nThe build process involves compressing the images, minifying the CSS and JS, MDX to MD conversion, HTML conversion and so on.\nThis uses a lot of read/write operations hence I think it would be a good test of the performance.\nSo I calculated the build time and the first render time of the blog on both systems to get a real feel of the performance.</p>\n<h3>Changing between Docker Desktop and Colima</h3>\n<p>I made sure to shut down Colima before starting Docker Desktop and vice versa.\nDocker desktop can be switched on/off from the UI.</p>\n<h3>Commands Used</h3>\n<h4>Colima stop and delete settings</h4>\n<pre><code>colima stop\ncolima delete\n</code></pre>\n<h4>Start Colima</h4>\n<p>To start Qemu mode</p>\n<pre><code>colima start --cpu 4 --memory 8 --arch aarch64 --vm-type qemu\n</code></pre>\n<p>To start VZ mode</p>\n<pre><code>colima start --cpu 4 --memory 8 --arch aarch64 --vm-type=vz --vz-rosetta\n</code></pre>\n<h2>Results</h2>\n<h3>Write Performance</h3>\n<h4>Docker Desktop</h4>\n<table>\n<thead><tr>\n<th>File size</th>\n<th>Duration (Worst)</th>\n<th>Duration (Best)</th>\n</tr></thead>\n<tbody>\n<tr>\n<td>1 MB</td>\n<td>311 ms</td>\n<td>227 ms</td>\n</tr>\n<tr>\n<td>10 MB</td>\n<td>1998 ms (2s)</td>\n<td>1943 ms (2 s)</td>\n</tr>\n<tr>\n<td>64 MB</td>\n<td>14433 ms (14 s)</td>\n<td>12016 ms (12 s)</td>\n</tr>\n<tr>\n<td>128 MB</td>\n<td>24745 ms (24 s)</td>\n<td>23455 ms (23 s)</td>\n</tr>\n<tr>\n<td>256 MB</td>\n<td>58937 ms (1 min)</td>\n<td>52252 ms (52 s)</td>\n</tr>\n<tr>\n<td>512 MB</td>\n<td>114213 ms (1.9 min)</td>\n<td>110060 ms (1.8 min)</td>\n</tr>\n<tr>\n<td>1024 MB</td>\n<td>262955 ms (4.38)</td>\n<td>194817 ms (3.2 min)</td>\n</tr>\n</tbody>\n</table>\n<h4>Colima (Qemu)</h4>\n<table>\n<thead><tr>\n<th>File Size</th>\n<th>Duration (Worst)</th>\n<th>Duration (Best)</th>\n</tr></thead>\n<tbody>\n<tr>\n<td>1 MB</td>\n<td>573 ms</td>\n<td>286 ms</td>\n</tr>\n<tr>\n<td>10 MB</td>\n<td>3077 ms (3s)</td>\n<td>2345 ms</td>\n</tr>\n<tr>\n<td>64 MB</td>\n<td>19087 ms (19 s)</td>\n<td>14116 ms</td>\n</tr>\n<tr>\n<td>128 MB</td>\n<td>38191 ms (38 s)</td>\n<td>26096 ms</td>\n</tr>\n<tr>\n<td>256 MB</td>\n<td>81071 ms (1.35 min)</td>\n<td>67062 ms</td>\n</tr>\n<tr>\n<td>512 MB</td>\n<td>151242 ms (2.52 min)</td>\n<td>159663 ms</td>\n</tr>\n<tr>\n<td>1024 MB</td>\n<td>293370 ms (4.89 min)</td>\n<td>301629 ms</td>\n</tr>\n</tbody>\n</table>\n<h4>Colima (VZ + Rosetta 2)</h4>\n<table>\n<thead><tr>\n<th>File size</th>\n<th>Duration (worst)</th>\n<th>Duration (best)</th>\n</tr></thead>\n<tbody>\n<tr>\n<td>1 MB</td>\n<td>291 ms</td>\n<td>236 ms</td>\n</tr>\n<tr>\n<td>10 MB</td>\n<td>2113 ms (2s)</td>\n<td>2199 ms</td>\n</tr>\n<tr>\n<td>64 MB</td>\n<td>12453 ms (12 s)</td>\n<td>12262 ms</td>\n</tr>\n<tr>\n<td>128 MB</td>\n<td>25315 ms (25 s)</td>\n<td>24603 ms</td>\n</tr>\n<tr>\n<td>256 MB</td>\n<td>49837 ms (49 min)</td>\n<td>50475 ms</td>\n</tr>\n<tr>\n<td>512 MB</td>\n<td>101883 ms (1.69 min)</td>\n<td>100692 ms</td>\n</tr>\n<tr>\n<td>1024 MB</td>\n<td>198126 ms (3.30 min)</td>\n<td>200163 ms</td>\n</tr>\n</tbody>\n</table>\n<h3>Read Performance</h3>\n<p>Shockingly, the read performance of Docker Desktop is very bad compared to Colima with a peak of <code>13 reads/ms</code>.\nColima with <code>Qemu</code> has a peak of <code>729 reads/ms</code> and Colima with <code>VZ</code> has a peak of <code>705 reads/ms</code>\nwhich is almost comparable.</p>\n<p>Surprisingly, the read performance of <code>Qemu</code> is better than <code>VZ</code>.</p>\n<h4>Docker Desktop</h4>\n<table>\n<thead><tr>\n<th>number of random reads</th>\n<th>Duration (Total)</th>\n<th>Average Speed</th>\n</tr></thead>\n<tbody>\n<tr>\n<td>1,000,000</td>\n<td>80593 - 81941 ms</td>\n<td>12 reads/ms</td>\n</tr>\n<tr>\n<td>500,000</td>\n<td>38238 - 39058 ms</td>\n<td>12 - 13 reads/ms</td>\n</tr>\n<tr>\n<td>200,000</td>\n<td>15299 - 15368 ms</td>\n<td>12 - 13 reads/ms</td>\n</tr>\n<tr>\n<td>100,000</td>\n<td>7755 - 7895 ms</td>\n<td>12 reads/ms</td>\n</tr>\n<tr>\n<td>10,000</td>\n<td>726 - 876 ms</td>\n<td>11 - 12 reads/ms</td>\n</tr>\n<tr>\n<td>1,000</td>\n<td>81 - 107 ms</td>\n<td>9 - 12 reads/ms</td>\n</tr>\n</tbody>\n</table>\n<p>I ran this 5 times and even restarted the docker engine multiple times but the results were the same.\nIf anyone knows why this is happening, please let me know.</p>\n<h4>Colima (Qemu)</h4>\n<table>\n<thead><tr>\n<th>number of random reads</th>\n<th>Duration (Total)</th>\n<th>Average Speed</th>\n</tr></thead>\n<tbody>\n<tr>\n<td>1,000,000</td>\n<td>1415 - 1419 ms</td>\n<td>704 - 707 reads/ms</td>\n</tr>\n<tr>\n<td>500,000</td>\n<td>686 - 695 ms</td>\n<td>696 - 719 reads/ms</td>\n</tr>\n<tr>\n<td>200,000</td>\n<td>276 - 278 ms</td>\n<td>719 - 728 reads/ms</td>\n</tr>\n<tr>\n<td>100,000</td>\n<td>137 - 141 ms</td>\n<td>709 - 724 reads/ms</td>\n</tr>\n<tr>\n<td>10,000</td>\n<td>15 - 17 ms</td>\n<td>666 - 729 reads/ms</td>\n</tr>\n<tr>\n<td>1,000</td>\n<td>2 - 3 ms</td>\n<td>333 - 500 reads/ms</td>\n</tr>\n</tbody>\n</table>\n<h4>Colima (VZ + Rosetta 2)</h4>\n<table>\n<thead><tr>\n<th>number of random reads</th>\n<th>Duration (Total)</th>\n<th>Average Speed</th>\n</tr></thead>\n<tbody>\n<tr>\n<td>1,000,000</td>\n<td>1493 - 1507 ms</td>\n<td>663 - 669 reads/ms</td>\n</tr>\n<tr>\n<td>500,000</td>\n<td>709 - 721 ms</td>\n<td>693 - 705 reads/ms</td>\n</tr>\n<tr>\n<td>200,000</td>\n<td>284 - 286 ms</td>\n<td>699 - 704 reads/ms</td>\n</tr>\n<tr>\n<td>100,000</td>\n<td>143 ms</td>\n<td>699 reads/ms</td>\n</tr>\n<tr>\n<td>10,000</td>\n<td>15 - 16 ms</td>\n<td>625 - 666 reads/ms</td>\n</tr>\n<tr>\n<td>1,000</td>\n<td>1 -2 ms</td>\n<td>500 - 1000 reads/ms</td>\n</tr>\n</tbody>\n</table>\n<h3>Print Performance</h3>\n<p><code>Docker Desktop</code> has the best print performance of all the three with a peak of <code>4074 chars/ms</code>.\n<code>Colima</code> with <code>VZ</code> comes second with a peak of <code>2984 chars/ms</code>.</p>\n<h4>Docker Desktop</h4>\n<table>\n<thead><tr>\n<th>number of lines to print</th>\n<th>number of chars printed</th>\n<th>total time</th>\n<th>Avg (per line)</th>\n<th>Avg (per character)</th>\n</tr></thead>\n<tbody>\n<tr>\n<td>10,000</td>\n<td>4,997,048</td>\n<td>1346 ms</td>\n<td>7 lines/ms</td>\n<td>3712 chars/ms</td>\n</tr>\n<tr>\n<td>100,000</td>\n<td>49.999,881</td>\n<td>12272 ms</td>\n<td>8 lines/ms</td>\n<td>4074 chars/ms</td>\n</tr>\n<tr>\n<td>1,000,000</td>\n<td>500,324,217</td>\n<td>122849 ms</td>\n<td>8 lines/ms</td>\n<td>4072 chars/ms</td>\n</tr>\n</tbody>\n</table>\n<h4>Colima (Qemu)</h4>\n<table>\n<thead><tr>\n<th>number of lines to print</th>\n<th>number of chars printed</th>\n<th>total time</th>\n<th>Avg (per line)</th>\n<th>Avg (per character)</th>\n</tr></thead>\n<tbody>\n<tr>\n<td>10,000</td>\n<td>5,013,996</td>\n<td>2019 ms</td>\n<td>4 lines/ms</td>\n<td>2483 chars/ms</td>\n</tr>\n<tr>\n<td>100,000</td>\n<td>50,028,339</td>\n<td>19253 ms</td>\n<td>5 lines/ms</td>\n<td>2598 chars/ms</td>\n</tr>\n<tr>\n<td>1,000,000</td>\n<td>500,593,068</td>\n<td>180024 ms</td>\n<td>5 lines/ms</td>\n<td>2780 chars/ms</td>\n</tr>\n</tbody>\n</table>\n<h4>Colima (VZ + Rosetta 2)</h4>\n<table>\n<thead><tr>\n<th>number of lines to print</th>\n<th>number of chars printed</th>\n<th>total time</th>\n<th>Avg (per line)</th>\n<th>Avg (per character)</th>\n</tr></thead>\n<tbody>\n<tr>\n<td>10,000</td>\n<td>5,030,577</td>\n<td>1752 ms</td>\n<td>5 lines/ms</td>\n<td>2871 chars/ms</td>\n</tr>\n<tr>\n<td>100,000</td>\n<td>50,010,997</td>\n<td>16755 ms</td>\n<td>5 lines/ms</td>\n<td>2984 chars/ms</td>\n</tr>\n<tr>\n<td>1,000,000</td>\n<td>499,828,875</td>\n<td>169597 ms</td>\n<td>5 lines/ms</td>\n<td>2947 chars/ms</td>\n</tr>\n</tbody>\n</table>\n<h3>Blog Performance</h3>\n<table>\n<thead><tr>\n<th>System</th>\n<th>Build time</th>\n<th>Initial rendering time</th>\n</tr></thead>\n<tbody>\n<tr>\n<td>Host</td>\n<td>673 - 711 s</td>\n<td>42.69 ms</td>\n</tr>\n<tr>\n<td>Colima (Qemu)</td>\n<td>817 - 836 s</td>\n<td>6.7 s</td>\n</tr>\n<tr>\n<td>Colima (VZ + Rosseta)</td>\n<td>671 - 924 s</td>\n<td>4.8 s</td>\n</tr>\n<tr>\n<td>Docker Desktop</td>\n<td>824 - 828 s</td>\n<td>5.6 s</td>\n</tr>\n</tbody>\n</table>\n<p>Compared to the host, the build time is almost the same for all the systems but <code>Colima</code> with <code>VZ</code> is the best among them\nwith the fastest build comparable to the host system.\nNone of them came close to the host system in terms of rendering time (as expected).</p>\n<h2>Summary</h2>\n<p><code>Docker Desktop</code> can be a better option for apps that require a lot of printing but fewer Read/Write operations.\nWhen it comes to extensive Read/Write operations,\n<code>Colima</code> with <code>VZ + Rosseta 2</code> would be my choice because it gives you better write and print performance than <code>Qemu</code>.\nFor my blog, I will be using <code>Colima</code> with <code>VZ + Rosseta 2</code> because it gives me the best build time and read/write performance.</p>\n",
            "enclosure": {},
            "categories": []
        },
        {
            "title": "Syncing your Elfeed saves between multiple systems",
            "pubDate": "2023-08-15 19:16:14",
            "link": "https://blog-schwiftycold.firebaseapp.com/blog/2023-08-15-elfeed-sync-between-multiple-systems/",
            "guid": "https://blog-schwiftycold.firebaseapp.com/blog/2023-08-15-elfeed-sync-between-multiple-systems/",
            "author": "",
            "thumbnail": "",
            "description": "Elfeed is a great RSS reader for Emacs, but it doesn't have a built-in way to sync your feeds between multiple systems. Here's how I do it.",
            "content": "\n<h2>Introduction</h2>\n<p>I recently started using <code>Emacs</code> with <code>Doom Emacs</code> as my primary text editor.\nI've been using <code>VSCode</code> for years, but I've always been curious about <code>Emacs</code> and I wanted to give it a try.\nI'm still exploring different modules, and I'm really enjoying the beginner's keybindings<code>&lt;SPC&gt; h r r</code> and <code>&lt;SPC&gt; q R</code>.\nI was in an endless loop of searching for a good RSS reader for my needs.\nI tried some Android-based, web-based, and desktop-based readers, but I couldn't find one that satisfied my need.</p>\n<p>I wanted a reader that I can switch to easily in my free time.\nI wanted a reader that I can sync b/w my devices (which doesn't include mobile devices as I don't use them for reading).\nI wanted a reader that can tag my feeds according to my customizations.\nAnd most important of all, I wanted a simple reader that doesn't have a lot of features that I don't need.</p>\n<p>I found <code>Elfeed</code> to be the best RSS reader for my needs.\nIt's simple, it's fast, it's customizable, and it's easy to switch to whenever you're free.</p>\n<p>The only thing that I didn't like about <code>Elfeed</code> is that it doesn't have a built-in way to sync your feeds between multiple systems.\nBut there are some workarounds that you can use to sync your feeds.</p>\n<h2>Data storage in Elfeed</h2>\n<p>Let's first understand how Elfeed stores your data.</p>\n<h3>Data storage location</h3>\n<p>By default, <code>Elfeed</code> stores your data inside <code>~/.elfeed</code> directory.\nYou can change this location by setting the <code>elfeed-db-directory</code> variable.</p>\n<p>I'm using <code>Doom Emacs</code>, so I set this variable inside my <code>config.el</code> file.\nYou can do the same with your installation depending on what package manager you use.</p>\n<pre><code>(after! elfeed\n  (setq elfeed-db-directory \"~/.elfeed-data\"))\n)\n</code></pre>\n<h3>Data storage format</h3>\n<pre><code>ls -la ~/.elfeed-data\ndrwxrwxrwx  106 shubham.kumar  1729907015    3392 Aug 15 22:28 data\n-rwxrwxrwx@   1 shubham.kumar  1729907015  924723 Aug 15 22:52 index\n\nls -a ~/.elfeed-data/data\n00    21    37  ...  f0\n06    24    3c  ...  f2\n09    27    3e  ...  ff\n\nls ~/.elfeed-data/data/00\n00e8db47f3a5b93b0fbb9b4c31748f607ae7bae5\n</code></pre>\n<p>The <code>index</code> file is a binary file that contains the metadata of your feeds.\nThe metadata includes the title, link, tags, etc.\nThis means that the <code>index</code> file contains the data that you see when you open <code>Elfeed</code> in <code>Emacs</code>.</p>\n<p>The <code>data</code> directory contains the actual data of your feeds which are your list of blogs.\nThe <code>data</code> directory contains the hash of the feed as the name of the file.</p>\n<p>And just like <code>git</code> file storage, the <code>data</code> directory contains subdirectories named with the\nfirst two characters of the hash and the rest of the hash as a subdirectory inside that.</p>\n<blockquote>\n<p>For more detailed information about the data storage format, you can check <a href=\"https://nullprogram.com/blog/2013/09/09/\">the docs by the creator, Chris Wellons</a>.</p>\n</blockquote>\n<h2>What to sync?</h2>\n<p>Looking at the storage format of <code>Elfeed</code>, we can say that we can sync our\nfeeds by syncing the <code>index</code> file and the <code>data</code> directory.\nBut the <code>data</code> file will be generated based on your feeds,\nso we don't need to sync it if you are already syncing your list of blogs.</p>\n<p>Plus comparing the size of <code>org</code> file (1.6K) and <code>data</code> directory (3.3K),\nwe can see that the data directory is much larger than the org file.\nSo I think it's better to sync the <code>index</code> file and the <code>org</code> file that contains\nyour list of blogs than to sync the entire <code>elfeed-data</code> directory.</p>\n<pre><code>ls -lh ~/.elfeed-data/\ntotal 1808\ndrwxrwxrwx  106 shubham.kumar  1729907015   3.3K Aug 15 22:28 data\n-rwxrwxrwx@   1 shubham.kumar  1729907015   903K Aug 15 22:52 index\n\nls -lh ~/Documents/org/elfeed.org\n-rwxrwxrwx  1 shubham.kumar  1729907015   1.6K Aug 13 08:00 /Users/shubham.kumar/Documents/org/elfeed.org\n</code></pre>\n<p>For me, I'm using <code>org-mode</code> to store my list of blogs that are being synced using version control.\nSo whenever I start using <code>Emacs</code> on my other system, I always update the org files.\nThis ensures my list of blogs is always up to date.</p>\n<p>So the only thing I need now is to sync the metadata of my feeds.\nAnd for that, I'm using <code>Syncthing</code>.</p>\n<h2>Syncing with Syncthing</h2>\n<h3>My current setup</h3>\n<p>I have 2 systems that I use regularly.</p>\n<ul>\n<li>My primary system is my <code>Linux Mint</code> system which I use for personal stuff.</li>\n<li>My secondary system is my <code>MacBook Air M1</code> which I use for work.</li>\n</ul>\n<p>On both systems, I use <code>Doom Emacs</code> and <code>Elfeed</code>.</p>\n<h3>Initial syncing (Syncthing on both systems)</h3>\n<p>&lt;!plantuml/&gt;</p>\n<p>This ensures that whenever one of my systems is offline, the other system can sync with my phone.\nAnd when both systems are online, they can sync with each other or my phone.\nThis will ensure that the <code>index</code> file is always synced between the systems.</p>\n<h4>Problem with this approach</h4>\n<p>The problem with this approach is that I now have to dedicate some amount of storage to my <code>index</code> file.\nSo I now have an unutilized <code>index</code> file on my phone which is taking up space.\nAnd I have to make sure that my phone is always connected to the internet and is always running <code>Syncthing</code>.\nThis will also impact my battery life.</p>\n<p>I have to check how much effect this has on my battery life (will update the blog after calculating this).</p>\n<h3>Best approach (Using Raspberry Pi)</h3>\n<p>I haven't implemented this yet, but I think this is the best approach.\nIf I get any problem with my current setup, I'll implement this.</p>\n<p>I'll buy a Raspberry Pi and install <code>Syncthing</code> on it.\nI'll add the <code>~/.elfeed-data</code> directory to the sync list while ignoring the <code>data</code> directory.\nI'll add both systems to sync with my Raspberry Pi and with each other.\nI'll ensure that my Raspberry Pi is always connected to the internet and is always running <code>Syncthing</code>.\nI'll also have to port forward my Raspberry Pi so that I can access it from outside my network.\nFor this, I may need to talk to my ISP about static IP and port forwarding capabilities.</p>\n<p>Will write a blog about this if I implement this.</p>\n<h2>Conclusion</h2>\n<p>In this post, I talked about how I sync my <code>Elfeed</code> feeds between my systems.\nI talked about the data storage format of <code>Elfeed</code> and how I sync my feeds using <code>Syncthing</code>.\nI also talked about the problems with my current setup and how I can improve it by introducing a new system to sync files.\nIf you are using <code>Emacs</code>, I highly recommend you try <code>Elfeed</code> as your RSS reader.\nAnd if you are already using <code>Elfeed</code>, I hope this post helps you to sync your feeds between your systems.</p>\n",
            "enclosure": {},
            "categories": []
        },
        {
            "title": "Inter-language communication between Erlang (rebar3) and Python using Erlport",
            "pubDate": "2023-08-12 23:40:40",
            "link": "https://blog-schwiftycold.firebaseapp.com/blog/2023-08-13-erlang-python-erlport/",
            "guid": "https://blog-schwiftycold.firebaseapp.com/blog/2023-08-13-erlang-python-erlport/",
            "author": "",
            "thumbnail": "",
            "description": "Creating a simple program to communicate between Erlang and Python using Erlport",
            "content": "\n<p>When it comes to fault-tolerant systems, there are very few languages that can beat <code>Erlang</code>.\nWhile <code>Erlang</code> is a great language for building fault-tolerant systems, it is not the best language for building AI/ML applications.\nThere are instances where you would want to use <code>Python</code> for building AI/ML applications and <code>Erlang</code> for building fault-tolerant systems.\nIn such cases, you would want to use <code>Erlang</code> and <code>Python</code> together.\nThis is where <a href=\"http://erlport.org/\">Erlport library</a> comes in.</p>\n<p>But before understanding Erlport, you need an understanding of <code>ports</code> in Erlang.</p>\n<h2>Ports</h2>\n<p>In simple terms, <code>ports</code> are used to communicate with external programs which are not written in Erlang.\nSo if you have a C program or a Python program and you want to communicate with it from Erlang, you can use <code>ports</code>.\nIt's not a restriction that you can only use <code>ports</code> to communicate with external programs, you can also use <code>ports</code> to communicate with other Erlang nodes.\n<code>Port</code> provides you with a byte-oriented interface.\nWhich means, you send a list of bytes and receive a list of bytes during the communication.\nThis also means you need to handle the encoding and decoding of the data at both ends by yourself.</p>\n<h2>Erlport</h2>\n<p>Erlport is a library that internally uses the <code>port</code> mechanism for communication and provides a wrapper around it.\nThis makes the integration of Erlang and Python very easy.\nRight now, Erlport supports Python and Ruby.\nIn this article, we will be focusing on Erlang and Python implementation.</p>\n<h3>Installation</h3>\n<p>If you are using <code>rebar3</code> for building your Erlang application, you can add the following dependency to your <code>rebar.config</code> file.</p>\n<pre><code>...\n{deps, [\n    erlport\n]}.\n...\n</code></pre>\n<p>Or you can use the GitHub link for the dependency.</p>\n<pre><code>{erl_opts, [debug_info]}.\n{deps, [\n  {erlport, {git, \"https://github.com/erlport/erlport.git\", {tag, \"v0.10.1\"}}}\n]}.\n</code></pre>\n<p>You'll also need to tell <code>Erlang</code> to wait for the dependencies to be available before starting the app.\nThis can be done by adding the entry in <code>*.app.src</code> file.</p>\n<pre><code> {applications,\n   [kernel,\n    stdlib,\n    erlport\n   ]},\n</code></pre>\n<h3>Python program</h3>\n<p>Let's create an <code>add</code> program in Python and keep the file in the <code>priv</code> directory.</p>\n<pre><code>def add(a, b):\n    return a + b\n</code></pre>\n<h3>Erlang program</h3>\n<p>The <code>Erlang</code> program will initialize the Python port using <code>python:start/1</code> function.\nAnd will call the <code>add</code> function of the Python program using <code>python:call/4</code> function.\nThe <code>python:call/4</code> function takes the <code>Pid</code> of the Python port, the module name, the function name, and the list of arguments to be passed to the function.</p>\n<p>Here, my <code>Python</code> module is named, <code>math</code> and the function is <code>add</code> with <code>2</code> and <code>4</code> as parameters.</p>\n<pre><code>-module(main).\n-export([call_python/0]).\n\ncall_python() -&gt;\n  PythonCodePath = code:priv_dir(aiml_model_wrapper),\n  {ok, P} = python:start([{python_path, PythonCodePath}, {python, \"python3\"}]),\n  python:call(P, math, add, [2, 4]).\n\n</code></pre>\n<h3>Running the program</h3>\n<p>Running the program is as simple as compiling and running the <code>Erlang</code> program.</p>\n<pre><code>reabr3 compile\nreabr3 shell\n1&gt; main:call_python().\n</code></pre>\n<h2>Conclusion</h2>\n<p>In this article, we saw how we can use <code>Erlang</code> and <code>Python</code> together using <code>Erlport</code>.\nThis is a very simple example of how you can use <code>Erlang</code> and <code>Python</code> together.\nYou can use this to build complex systems where you can use <code>Erlang</code> for building fault-tolerant systems and <code>Python</code> for building AI/ML applications.\nAt <a href=\"https://www.greyorange.com/\">GreyOrange</a> I created an Erlang wrapper for our AI/ML models using the same approach.</p>\n",
            "enclosure": {},
            "categories": []
        },
        {
            "title": "Download files via REST API using chunked transfer encoding",
            "pubDate": "2023-05-26 01:04:53",
            "link": "https://blog-schwiftycold.firebaseapp.com/blog/2023-05-26-chunked-file-transfer-protocol-rest-api/",
            "guid": "https://blog-schwiftycold.firebaseapp.com/blog/2023-05-26-chunked-file-transfer-protocol-rest-api/",
            "author": "",
            "thumbnail": "",
            "description": "Here, we will create a Python server that will send us a heuristic file in chunks and a Java client will read these files and download them over the REST API using chunked transfer encoding.",
            "content": "\n<p>Chunked transfer encoding is a protocol to send data in chunks over HTTP.\nThis allows us to transfer a large amount of data in chunks of small size.</p>\n<p>With chunked encoding, the server splits the response into a series of smaller \"chunks\" of data. Each chunk includes a size indicator followed by the actual chunk data. The size indicator specifies the length of the chunk data in bytes. The chunks are sent to the client one by one and can be processed by the client as they arrive.</p>\n<p>Here, we will create a Python server that will send us a heuristic file in chunks and a Java client will read these files and download them over the REST API.</p>\n<h2>Project structure</h2>\n<p>The project is a Maven project at root containing a sub-directory for our Python server.</p>\n<pre><code> tree .\n.\n|-- README.MD\n|-- pom.xml\n|-- python\n|   |-- requirements.txt\n|   `-- server.py\n`-- src\n    `-- main\n        `-- java\n            `-- com\n                `-- gor\n                    `-- poc\n                        |-- App.java\n                        `-- FileDownloader.java\n\n\n</code></pre>\n<h2>Python server</h2>\n<p>This is a Flask server that exposes a <code>download</code> route.</p>\n<p>For a POC, we will just send a particular file called 'heuristics' whenever a request is made to the server.\nThe server will send the files in chunks of <code>1024</code> which can be configured as per your need.</p>\n<pre><code># Define the route\n@app.route('/download')\ndef download_file():\n    # This is the file we want to download whenever a client requests\n    file_dir = 'python/public/stream/data/'\n    file_name = \"heuristics.bin\"\n    file_path = file_dir + file_name\n    chunk_size = 1024\n    \n    # Create chunks and yield them whenever required\n    def generate():\n        # Chunks size of each chink of data     \n        with open(file_path, 'rb') as file:\n            while True:\n                chunk = file.read(chunk_size)\n                if not chunk:\n                    break\n                yield chunk\n\n    response = Response(\n\n        # stream_with_context is a Flask functionality to send files in chunks\n        # You can read more about it here https://flask.palletsprojects.com/en/1.0.x/patterns/streaming/\n        stream_with_context(generate()),\n        mimetype='application/octet-stream'\n    )\n    response.headers.set('Content-Disposition', 'attachment', filename=file_name)\n    return response\n</code></pre>\n<pre><code># Let's create a heuristic file for our server to send \n# We'll create a random file of size 1GB\ndd if=/dev/urandom of=python/public/stream/data/heuristics.bin bs=1G count=1\n</code></pre>\n<pre><code># The below commands only works on Mac OS\n# Check the size of the file generated in bytes\nstat -f \"%z\" python/public/stream/data/heuristics.bin\n# 1073741824\n\n# Let's check the md5 of the file\nmd5 -r python/public/stream/data/heuristics.bin | awk '{print $1}'\n# c5b8959732d3359791bcd06ca5a92dc2\n</code></pre>\n<pre><code># For linux, you can use the below code\n# Check the size of the file generated in bytes\nstat -c \"%s\" python/public/stream/data/heuristics.bin\n\n# Check the md5 of the file\nmd5sum python/public/stream/data/heuristics.bin | awk '{print $1}'\n\n</code></pre>\n<pre><code># Let's run the python server now\n# We'll use a virtual environment to run the server (You can also use conda))\nsource .venv/bin/activate\n\n# Install the dependencies\npip install -r python/requirements.txt\n\n# Run the server (and keep it running)\npython python/server.py\n</code></pre>\n<p>We can also use <code>curl</code> to check the header.</p>\n<p>The presence of <code>Transfer-Encoding: chunked</code> tells us that the server is using the chunked protocol to send the data.</p>\n<pre><code># curl the header\n# -I asks for headers only \n# -XGET says it to use GET method (can be omited as it is the default)\ncurl -I -XGET \"http://localhost:5000/download\"\n\n# HTTP/1.1 200 OK\n# Server: Werkzeug/2.3.4 Python/3.10.10\n# Date: Fri, 26 May 2023 00:52:01 GMT\n# Content-Type: application/octet-stream\n# Content-Disposition: attachment; filename=heuristics.bin\n# X-Chunk-Size: 1024\n# Transfer-Encoding: chunked\n# Connection: close\n</code></pre>\n<p>You can also use <code>curl</code> to download the file.</p>\n<pre><code># Download the file in the current directory\n# First let's remove the file if it exists\nrm -rf heuristics.bin\n\n# -O says to save the file with the same name as the server\n# -J says to use the filename from the server\n# -L says to follow redirects (for us it's optional as we are not using any redirects)\ncurl -O -J -L http://localhost:5000/download\n\n# Also let's verify the file size and md5 of the downloaded file\nstat -f \"%z\" heuristics.bin\n# 1073741824\n\nmd5 -r heuristics.bin | awk '{print $1}'\n# c5b8959732d3359791bcd06ca5a92dc2\n</code></pre>\n<p>You can match the <code>md5</code> of the server file and what we downloaded to check the successful file transfer.</p>\n<h2>Java Client</h2>\n<p>The code for downloading the file using <code>Java</code> is located at <code>com.gor.poc.FileDownloader</code>.</p>\n<p>Below is the explanation of the code.</p>\n<pre><code>// This is the REST API which will serve you the file\nString fileUrl = \"http://localhost:5000/download\";\nString savePath = \"\"; \n\ntry {\n    // Defining the URL\n    URL url = new URL(fileUrl);\n    HttpURLConnection connection = (HttpURLConnection) url.openConnection();\n    connection.setRequestMethod(\"GET\");\n\n    int responseCode = connection.getResponseCode();\n    if (responseCode == HttpURLConnection.HTTP_OK) {\n        // Get the file name from the header\n        String fileName = connection.getHeaderField(\"Content-Disposition\");\n        fileName = fileName.substring(fileName.lastIndexOf(\"=\") + 1);\n        String filePath = savePath + \"/\" + fileName;\n\n        // Create an Input stream to download the file\n        InputStream inputStream = connection.getInputStream();\n\n        // This is where the file will be saved\n        FileOutputStream outputStream = new FileOutputStream(\"./\"+filePath);\n\n        // Read the data in chunks and save it to the file\n        byte[] buffer = new byte[4096];\n        int bytesRead;\n        while ((bytesRead = inputStream.read(buffer)) != -1) {\n            outputStream.write(buffer, 0, bytesRead);\n        }\n        \n        outputStream.close();\n        inputStream.close();\n\n        System.out.println(\"File downloaded successfully!\");\n    } else {\n        System.out.println(\"File download failed. Server returned response code: \" + responseCode);\n    }\n\n    connection.disconnect();\n} catch (IOException e) {\n    e.printStackTrace();\n}\n</code></pre>\n<pre><code># You can run the java program using maven \n# First remove the file if it exists\nrm -rf heuristics.bin\n\n# Compile and run the java program\nmvn clean compile\nmvn exec:java -Dexec.mainClass=\"com.gor.poc.FileDownloader\"\n# [INFO] Scanning for projects...\n# [INFO] \n# [INFO] --------------------&lt; com.gor.poc:stream_download &gt;---------------------\n# [INFO] Building stream_download 1.0-SNAPSHOT\n# [INFO] --------------------------------[ jar ]---------------------------------\n# [INFO] \n# [INFO] --- exec-maven-plugin:3.1.0:java (default-cli) @ stream_download ---\n# Downloading file in chunks...\n# 27 bytes read\n# 997 bytes read\n# 25 bytes read\n# 999 bytes read\n# 25 bytes read\n\n# Get the size and md5 of the downloaded file\nstat -f \"%z\" heuristics.bin\nmd5 -r python/public/stream/data/heuristics.bin | awk '{print $1}'\n</code></pre>\n<p>The output shows us that the files are downloaded in small chunks like 27 bytes, 997 bytes even though our client is reading a buffer of size 4096.\nThis is because the server is sending chunks at the rate of 1024 bytes which is being consumed by the Java program much faster.\nThis is one disadvantage of using chunk encoding where the client has no control over the data transfer.\nEven though the client is faster, the download is dependent on the server configurations.</p>\n<p>To solve this let's configure our Python server to send 1 MB chunks instead of 1 KB. Restart your Python server.</p>\n<pre><code>    file_path = file_dir + file_name\n-    chunk_size = 1024  # Adjust the chunk size as per your requirements\n+    chunk_size = 1024 * 1024\n    \n    def generate():\n</code></pre>\n<pre><code>rm -rf heuristics.bin\n\n# Compile and run the java program\nmvn clean compile\nmvn exec:java -Dexec.mainClass=\"com.gor.poc.FileDownloader\"\n# Downloading file in chunks...\n# 24 bytes read\n# 4096 bytes read\n# 4096 bytes read\n# 4096 bytes read\n</code></pre>\n<p>Now the file transfer is much faster but still limited because the client is consuming the data at a slower rate than what the server is sending.\nWe would have increased the file transfer rate by configuring the client to read 1 MB.</p>\n<p>Let's configure our client to use the exact bytes our server is sending.</p>\n<p>For this, we'll send the chunk size used by our server as header metadata.</p>\n<pre><code>    response.headers.set('Content-Disposition', 'attachment', filename=file_name)\n+   response.headers.set('X-Chunk-Size', str(chunk_size))  # Add chunk size as a header\n\n</code></pre>\n<p>And on the Java side, we can read this header and set our buffer size to match the chunk size.</p>\n<pre><code>    // This is the REST API which will serve you the file\n    String fileUrl = \"http://localhost:5000/download\"; \n    String savePath = \"\"; \n\n    try {\n      // Define the URL \n      URL url = new URL(fileUrl);\n      HttpURLConnection connection = (HttpURLConnection) url.openConnection();\n      connection.setRequestMethod(\"GET\");\n\n      int responseCode = connection.getResponseCode();\n      if (responseCode == HttpURLConnection.HTTP_OK) {\n        \n        // Get the file name from the header\n        String fileName = connection.getHeaderField(\"Content-Disposition\");\n        fileName = fileName.substring(fileName.lastIndexOf(\"=\") + 1);\n        String filePath = savePath + \"/\" + fileName;\n\n        // Get the chunk size from the response headers\n+       String chunkSizeHeader = connection.getHeaderField(\"X-Chunk-Size\");\n+       int chunkSize = Integer.parseInt(chunkSizeHeader);\n\n        // Create an inout stream to download the file\n        InputStream inputStream = connection.getInputStream();\n        FileOutputStream outputStream = new FileOutputStream(\"./\" + filePath);\n\n        // Read the data in chunks and save it to the file\n+       byte[] buffer = new byte[chunkSize];\n        int bytesRead;\n\n        // Check if the server supports chunked transfer encoding\n        String transferEncoding = connection.getHeaderField(\"Transfer-Encoding\");\n        boolean isChunked = \"chunked\".equalsIgnoreCase(transferEncoding);\n\n+        // Just to distinguish b/w chunked protocol and normal file transfer\n+        if (isChunked) {\n+          // Read and write the response data in chunks\n+          System.out.println(\"Downloading file in chunks...\");\n+          while ((bytesRead = inputStream.read(buffer)) != -1) {\n+            System.out.println(bytesRead + \" bytes read\");\n+            outputStream.write(buffer, 0, bytesRead);\n+          }\n+        } else {\n+          // Read and write the entire response data\n+          System.out.println(\"Downloading file as whole\");\n+          while ((bytesRead = inputStream.read(buffer)) != -1) {\n+            System.out.println(bytesRead + \" bytes read\");\n+            outputStream.write(buffer, 0, bytesRead);\n+          }\n+        }\n\n        outputStream.close();\n        inputStream.close();\n\n        System.out.println(\"File downloaded successfully!\");\n      } else {\n        System.out.println(\"File download failed. Server returned response code: \" + responseCode);\n      }\n\n      connection.disconnect();\n    } catch (IOException e) {\n      e.printStackTrace();\n    }\n</code></pre>\n<pre><code>rm -rf heuristics.bin\n\n# Compile and run the java program\nmvn clean compile\nmvn exec:java -Dexec.mainClass=\"com.gor.poc.FileDownloader\"\n\n# Downloading file in chunks...\n# 24 bytes read\n# 768664 bytes read\n# 279888 bytes read\n# 24 bytes read\n# 391942 bytes read\n# 393112 bytes read\n</code></pre>\n<p>Looks like we are now using the full capability on the client side.</p>\n<h2>Analysis</h2>\n<p>Now the only optimization we will require to send the files quickly is on the server side.\nLet's log the time it takes to download the file while using different chunk sizes on the server.</p>\n<table>\n<thead><tr>\n<th>Chunk size</th>\n<th>Time (in ms)</th>\n</tr></thead>\n<tbody>\n<tr>\n<td>512 B</td>\n<td>9337</td>\n</tr>\n<tr>\n<td>1 KB</td>\n<td>5150</td>\n</tr>\n<tr>\n<td>1 MB</td>\n<td>465</td>\n</tr>\n<tr>\n<td>64 MB</td>\n<td>639</td>\n</tr>\n<tr>\n<td>128 MB</td>\n<td>744</td>\n</tr>\n<tr>\n<td>256 MB</td>\n<td>740</td>\n</tr>\n<tr>\n<td>1 GB</td>\n<td>1169</td>\n</tr>\n</tbody>\n</table>\n<p>The time taken to transfer the file first decreases as we increase the chunk size and then start increasing on increasing the chunk size.</p>\n<p>The above analysis was done on Mac M1 - 2020 Model with 16 GB RAM (4 GB available).</p>\n",
            "enclosure": {},
            "categories": []
        },
        {
            "title": "Package Java JNI libraries in a JAR using Maven",
            "pubDate": "2023-05-02 11:51:43",
            "link": "https://blog-schwiftycold.firebaseapp.com/blog/2023-05-02-package-java-jni-libraries/",
            "guid": "https://blog-schwiftycold.firebaseapp.com/blog/2023-05-02-package-java-jni-libraries/",
            "author": "",
            "thumbnail": "",
            "description": "This article shows how to package JNI libraries in a JAR using Maven.",
            "content": "\n<h2>Introduction</h2>\n<p>If you are using a native library in Java, you can just load the <code>dll</code> or <code>dylib</code> or <code>so</code> files using <code>System.loadLibrary</code> method.\nBut if you are packaging your Java program as a JAR file, the native library will not be available to the <code>System.loadLibrary</code> method.\nIn this article, we'll see the correct way to package the native library in the JAR file using Maven so that it will be accessible in our Java program without any other user intervention.</p>\n<h2>Project Structure</h2>\n<p>Maven follows a specific project structure where all the source code is placed in <code>src/main/java</code> and all the resources are placed in <code>src/main/resources</code>.\nThe resources directory is added to the classpath when the project is compiled and packaged.\nThis means you can access the contents of the <code>resources</code> directory directly in the Java program by specifying the path relative to the resources directory.</p>\n<p>Let's say you have a native library <code>libhello.dylib</code> in the <code>resources/lib</code> directory.\nAfter packaging this will be available at the path <code>lib/libhello.dylib</code> in the JAR file.</p>\n<h2>Use Native Utils</h2>\n<p>The default way to load a native library in Java is to use the <code>System.loadLibrary</code> method.\nThe <code>maven package</code> command will generate the jar file of the project.\nThe native library will be packaged in the <code>lib</code> directory inside the jar file which will not be available to the <code>System.loadLibrary</code> method.</p>\n<p>Now running our app will require us to extract the library from the jar file and put it somewhere on the system.\nThis can be done using <a href=\"https://github.com/adamheinrich/native-utils\">Native Utils</a>.\nInternally this utility is extracting the native library from the jar file to a temporary directory and loads the library from there.\nThis will keep our hands out of the dirty work of extracting the library and loading it.\nIt provides a method <code>loadLibraryFromJar</code> which can be used to load the native library from the jar file.</p>\n<h2>Configurations</h2>\n<p>As <a href=\"https://github.com/adamheinrich/native-utils\">Native Utils</a> is not available as a release package, we'll need to add it directly from the GitHub repository.\nFor this, we can use the <a href=\"https://jitpack.io/\">JitPack</a> service which allows us to add GitHub repositories as Maven dependencies.\nYou'll need to add <a href=\"https://jitpack.io/\">JitPack</a> as a repository in your <code>POM.XML</code> file.</p>\n<pre><code>  &lt;repositories&gt;\n    &lt;repository&gt;\n      &lt;id&gt;jitpack.io&lt;/id&gt;\n      &lt;url&gt;https://jitpack.io&lt;/url&gt;\n    &lt;/repository&gt;\n  &lt;/repositories&gt;\n</code></pre>\n<p>Also, add the dependency for <a href=\"https://github.com/adamheinrich/native-utils\">Native Utils</a>.\nFor this, we need to give <code>groupId</code> as <code>com.github.&lt;user-name&gt;</code> and <code>artifactId</code> as <code>&lt;repository-name&gt;</code>.\nSince we are using the latest commit from the repository, we'll need to specify the commit hash in the version tag.\nBelow is how the dependency will look like in <code>POM.XML</code>.</p>\n<pre><code>  &lt;dependency&gt;\n    &lt;groupId&gt;com.github.adamheinrich&lt;/groupId&gt;\n    &lt;artifactId&gt;native-utils&lt;/artifactId&gt;\n    &lt;version&gt;e6a39489662846a77504634b6fafa4995ede3b1d&lt;/version&gt;\n  &lt;/dependency&gt;\n</code></pre>\n<p>Now, you can confirm the dependency is added by running the <code>maven dependency:tree</code> command.</p>\n<pre><code>&gt; mvn dependency:tree\n\n[INFO] --- maven-dependency-plugin:2.8:tree (default-cli) @ javaunsafe ---\n[INFO] com.gor.poc:javaunsafe:jar:1.0-SNAPSHOT\n[INFO] +- log4j:log4j:jar:1.2.17:compile\n[INFO] +- junit:junit:jar:3.8.1:test\n[INFO] \\- com.github.adamheinrich:native-utils:jar:e6a39489662846a77504634b6fafa4995ede3b1d:compile\n</code></pre>\n<p>You can see, my project is using 3 dependencies, log4j, junit, and native-utils with their version as specified.</p>\n<h2>Load the native library</h2>\n<p>In your Java file, you can call the <code>loadLibraryFromJar</code> method to load the native library.</p>\n<pre><code>import cz.adamh.utils.NativeUtils;\n\npublic class NativeMemoryLoader {\n\n  static {\n    try {\n      NativeUtils.loadLibraryFromJar(\"/lib/libhello.dylib\");\n    } catch (IOException e) {\n      // TODO Auto-generated catch block\n      e.printStackTrace();\n    }\n  }\n\n  public static native void sayHello();\n\n  ...\n}\n</code></pre>\n<h2>Load library based on your OS</h2>\n<p>The problem with loading native libraries like C/C++ is that your compiled file is dependent on the platform.\nThis means you'll need to compile the library for each platform and then package it in the JAR file.\nSo you'll have different versions of the library for different platforms. And you'll end up with <code>so</code>, <code>dylib</code>, and <code>dll</code> files in your <code>lib</code> directory.\nLet's modify our Java code to use the library based on the platform.</p>\n<pre><code>  // Get the file extension based on OS\n  String osName = System.getProperty(\"os.name\").toLowerCase();\n  String libExtension = osName.contains(\"win\") ? \".dll\" :\n                        osName.contains(\"mac\") ? \".dylib\" : \".so\";\n  String libPath = \"/lib/libhello\" + libExtension;\n  NativeUtils.loadLibraryFromJar(libPath);\n</code></pre>\n<h2>Conclusion</h2>\n<p>In this article, we saw how to package native libraries in a JAR file using Maven and load them in our Java program.\nWe also saw how to load the library based on the platform.</p>\n",
            "enclosure": {},
            "categories": []
        },
        {
            "title": "Configure Google Java Formatter with VSCode",
            "pubDate": "2023-04-22 05:07:34",
            "link": "https://blog-schwiftycold.firebaseapp.com/blog/2023-04-22-google-java-format-vscode/",
            "guid": "https://blog-schwiftycold.firebaseapp.com/blog/2023-04-22-google-java-format-vscode/",
            "author": "",
            "thumbnail": "",
            "description": "This blog is on how to configure Google Java Formatter with VSCode",
            "content": "\n<h2>Introduction</h2>\n<p>Using the same code format is helpful while working on a team project.\nThis helps you in code readability and also helps you to avoid merge conflicts and makes pull requests easier to focus on.\nFor this people use different code formatter like Google Java Formatter, Prettier, etc.\nIn this blog, we will see how to configure Google Java Formatter with VSCode.</p>\n<h2>Download the jar file</h2>\n<p>The first thing you need to do is download the Google Formatter jar file from <a href=\"https://github.com/google/google-java-format/releases\">here</a> and keep it in your home directory.\nYou can keep it anywhere you want but I prefer to keep it in my home directory.\nMake sure you download the jar file with <code>-all-deps</code> in the name.</p>\n<p>You can also download the jar file using the following command.\nThe <code>-O</code> flag is used to specify the location where you want to download the jar file.\nThe below command will download the jar file and store it in the home directory with the file name <code>google-code-formatter.jar</code>.</p>\n<pre><code>&gt; wget https://github.com/google/google-java-format/releases/download/v1.16.0/google-java-format-1.16.0.jar \\\n  -O ~/google-code-formatter.jar\n</code></pre>\n<h2>Configure the VSCode</h2>\n<p>Now we will configure the VSCode to use the Google Java Formatter.\nFor this, you'll need an extension called <a href=\"https://marketplace.visualstudio.com/items?itemName=SteefH.external-formatters\">External Formatters</a> extension.\nThis extension helps you use an external code formatter with VSCode.</p>\n<p>To install the extension, open the VSCode and press <code>Cmd + Shift + X</code> to open the extension tab.\nSearch for <code>External Formatters</code> authored by Stefan van der Haven and install it.</p>\n<p>Now open the VSCode settings by pressing <code>Cmd + ,</code> and search for <code>External Formatters</code>.\nThen click on the <code>Edit in settings.json</code> button for modifying the language setting.</p>\n<p>&lt;!image/&gt;</p>\n<p>Now add the following code in the <code>settings.json</code> file.</p>\n<pre><code>[\n  ...\n  \"externalFormatters.languages\": {\n      \"java\": {\n          \"command\": \"java\",\n          \"arguments\": [\n              \"-jar\",\n              \"/Users/shubham.kumar/google-java-format-1.16.0-all-deps.jar\",\n              \"-\"\n          ]\n      }\n  },\n  \"[java]\": {\n      \"editor.formatOnSave\": true,\n      \"editor.defaultFormatter\": \"SteefH.external-formatters\",\n      \"editor.tabSize\": 2\n  }\n]  \n</code></pre>\n<p>The above configuration defined inside <code>externalFormatters.languages</code> configures the External Formatters extension to use the Google Java Formatter jar file for formatting the Java files.\nThis configuration tells VScode to run <code>java -jar /Users/shubham.kumar/google-java-format-1.16.0-all-deps.jar &lt;your-file.java&gt;</code> command for formatting your java files.</p>\n<p>The <code>[java]</code> configuration is language specific setting that tells VSCode to use the External Formatters extension for formatting the Java files and run the formatter whenever a file is saved.\nGoogle Java Code formatter uses 2 spaces for indentation by default.\nSo we set the <code>editor.tabSize</code> to <code>2</code> for Java files.</p>\n<blockquote>\n<p>Note: You can also use <code>.vscode/settings.json</code> for configuring the VSCode settings for a particular project.</p>\n</blockquote>\n<h2>Conclusion</h2>\n<p>In this blog, we saw how to configure Google Java Formatter with VSCode.\nNow you can use the same code formatter for all your Java projects and also use it with your team members to avoid merge conflicts and make your pull requests easier to review.</p>\n",
            "enclosure": {},
            "categories": []
        },
        {
            "title": "Upgrade a simple HTML, CSS and JS site to a WebSocket application",
            "pubDate": "2023-01-26 20:53:46",
            "link": "https://blog-schwiftycold.firebaseapp.com/blog/2023-01-26-socketio-game/",
            "guid": "https://blog-schwiftycold.firebaseapp.com/blog/2023-01-26-socketio-game/",
            "author": "",
            "thumbnail": "",
            "description": "Initially, I thought it was going to be a short one but it ended up with more than 10 mins of reading time. This blog revolves around integrating a WebSocket app using socket.io to an existing HTML, CSS and JS site.",
            "content": "\n<h2>Intro</h2>\n<p>Sarvesh, one of my friends, created a gaming project called \"Pig Game\" during his Javascript learning journey.\nIt is a basic site built using HTML, CSS, and JS in which two people compete by rolling a die.</p>\n<p>At any moment, you have two options: roll the dice to increase your score or give up so that the next person may play.\nThe catch is that if the dice stop at <strong>1</strong>, your current score becomes <strong>0</strong>, and the game is passed to the next player.\nBut if you willingly pass the game to the next person, your current score gets added to the total score which ultimately saves your hard work.</p>\n<p>For the time being, this game is only a static web page with all of its capabilities running in the browser.\nAnd because of this, it is unplayable if you are not in the same room (or virtually sharing the screen of the host).\nWe want to make this a server-based game so that we may play together even from afar.</p>\n<p>Here, let's implement a WebSocket server using express.js &amp; socket.io. Also, we'll configure the front-end to establish a connection with this server.</p>\n<h2>HTTP vs WebSocket protocols</h2>\n<h3>HTTP</h3>\n<p>Let's have a bird-eye view of the traditional <code>HTTP</code> protocol.\nWith <code>HTTP</code>, a client sends a request to the server and the server process the request and sends a response in return.\nThe connection is then closed after this request-response cycle.\n<code>HTTP</code> is stateless and because it runs on TCP, data delivery is ensured.</p>\n<blockquote>\n<p>A stateless protocol does not require the server to retain information or status about each user for the duration of multiple requests. -- wikipedia</p>\n</blockquote>\n<p>You will be required to use cookies or server-side sessions to not forget the user in an <code>HTTP</code> protocol.</p>\n<p>&lt;!plantuml/&gt;</p>\n<p>I think that's all we need to talk about at least during the starting phase.</p>\n<h3>Steps 2 and 3: Onboarding the players</h3>\n<p>The first step is already taken care of by 'socket.io'.\nLet's configure our server for steps 2 and 3.\nWe want to send a 'ready' signal when 2 players are connected.\nIf more than 2 people join, we can reject their connection on the server side using <code>socket.disconnect()</code> function and send them a 'reject' signal.</p>\n<blockquote>\n<p><code>socket.emit()</code> sends the message to a particular socket (client), <code>socket.broadcast.emit()</code> sends the message to everyone except the socket (client) itself and <code>io.emit()</code> sends the message to all the clients connected to a socket.</p>\n</blockquote>\n<h4><code>server.js</code></h4>\n<pre><code>...\nlet players = {};    // This will store the details about the players like their score\nlet activePlayer = 0;\n\nio.on('connection', (socket) =&gt; {   \n  players[socket.id] = {    // Initialize the player on establishing a connection\n    \"socket\": socket,\n    \"total_score\": 0,\n    \"current_score\": 0,\n    \"isActivePlayer\": false,\n  } \n  let total_players = Object.keys(players).length\n  console.log(`Player with socket id ${socket.id} connected.`)\n  console.log(`Total number of players: ${total_players}`)\n\n  // Sending the connection status\n  // Reject if we already have 2 players connected\n  if (total_players &gt; 2) {\n    delete players[socket.id]\n    socket.emit(\"connection_status\", {\"connection_status\": \"reject\"});\n    socket.disconnect();\n    console.log(\"Already 2 players are onboarded\", Object.keys(players))\n  }\n  // Send ready signal when we have 2 players \n  else if (total_players == 2) {\n    // Tell player 1 to go first\n    activePlayer = 0\n    io.emit(\"connection_status\", {\"connection_status\": \"ready\", \"active_player\": Object.keys(players)[activePlayer]})\n  // Else send a waiting signal\n  else {\n    io.emit(\"connection_status\",  {\"connection_status\": \"waiting\"})\n  }\n\n  socket.on('disconnect', () =&gt; {\n      delete players[socket.id];\n      console.log('Client disconnected ' + socket.id);\n  });\n});\n</code></pre>\n<p>On the client side, we will consume the messages received to perform certain actions.\nThis can be done by adding a socket message listener, <code>socket.on()</code>.\nRight now, let's use alerts for notifying the current player about the game status.</p>\n<h4><code>public/script.js</code></h4>\n<pre><code>socket.on(\"connection_status\", (args) =&gt; {\n    console.log(\"Received a connection_status signal\", args.connection_status)\n    if (args.connection_status === 'waiting') {\n        alert(\"Waiting for second player to begin the game\")\n    }\n    else if (args.connection_status === 'ready') {\n        // Enable player is a function to activate and deactivate the roll and hold buttons\n        EnablePlayer(args.active_player);\n        alert(`Let's begin the game. ${args.active_player===socket.id ? \"It's your turn\" : \"Oponent's turn\"}`)\n    }\n    else if (args.connection_status === 'reject') {\n        alert(\"Can't join, already 2 players onboarded\")\n    }\n});\n</code></pre>\n<h3>Step 4: Play the game</h3>\n<p>Looks like this is all we wanted in the steps 2 and 3.\nFor step 4, we will send a signal from the client's side about his decision.\nThis can be achieved by the button event listners.\nAnd the server will calculate a random number b/w 1 to 6 and broadcast the response.</p>\n<h4><code>public/script.js</code></h4>\n<pre><code>...\n// Same for btnHold with decision \"hold\"\nbtnRoll.addEventListener('click', function(){\n    if(playing){\n        socket.emit(\"decide\", {\n            \"player_id\": socket.id,\n            \"decision\": \"roll\"\n         });\n    }\n});\n...\n</code></pre>\n<p>On the server, we can decide the score and broadcast it back to clients.\nWe will send the current and total scores of the player along with the next player's socket id and dice score.</p>\n<h4><code>server.js</code></h4>\n<pre><code>...\n function NotifyScores(active_player, dice) {\n    let player1 = Object.keys(players)[0];\n    let player2 = Object.keys(players)[1];\n\n    io.emit(\"score_update\", {\n      'player1_current': players[player1].current_score,\n      'player1_total': players[player1].total_score,\n      'player2_current': players[player2].current_score,\n      'player2_total': players[player2].total_score,\n      'next_turn': Object.keys(players)[activePlayer],\n      'acitive_player_roll': activePlayer,\n      'dice': dice\n    });\n  }\n\n  socket.on(\"decide\", (args) =&gt; {\n    console.log(\"decide\", args);\n    if (args.player_id === Object.keys(players)[activePlayer]) {\n      if (args.decision === 'roll'){\n        const dice = Math.trunc(Math.random()*6) + 1;\n        players[args.player_id].current_score = dice == 1 \n          ? 0 \n          : players[args.player_id].current_score + dice;\n        activePlayer = dice == 1 ? (activePlayer + 1) % 2 : activePlayer\n        NotifyScores(activePlayer, dice);\n      }\n      else if (args.decision === 'hold') {\n        players[args.player_id].total_score += players[args.player_id].current_score;\n        players[args.player_id].current_score = 0; \n        activePlayer = (activePlayer + 1) % 2;\n        NotifyScores(activePlayer, 0);\n      }\n    }\n  });\n</code></pre>\n<h3>Step 5: Update the scores</h3>\n<p>Step 5 is very simple.\nJust update the data received by the client from the server in step 4.</p>\n<h4><code>public/script.js</code></h4>\n<pre><code>socket.on(\"score_update\", (args) =&gt;{\n    console.log(\"score update\", args);\n    document.getElementById(`current--0`).textContent = args.player1_current;\n    document.getElementById(`current--1`).textContent = args.player2_current;\n    document.getElementById(`score--0`).textContent = args.player1_total;\n    document.getElementById(`score--1`).textContent = args.player2_total;\n    diceEl.classList.remove('hidden');\n    if (args.dice != 0) diceEl.src = `assets/images/dice-${args.dice}.png`;\n    EnablePlayer(args.next_turn)\n});\n</code></pre>\n<h3>Step 6: Decide the winner</h3>\n<p>Step 6, is deciding the winner.\nWhoever scores greater than 60 will be entitled the winner.\nWe have many options to send this information.\nThis can be sent by the server as a new event, \"winner\".\nOr we can include this with the 'score_update' event.</p>\n<p>Let's update the \"decide\" event to send a new message whenever a player wins.</p>\n<h4><code>server.js</code></h4>\n<pre><code> socket.on(\"decide\", (args) =&gt; {\n    if (args.player_id === Object.keys(players)[activePlayer]) {\n      ...\n      else if (args.decision === 'hold'){\n        ...\n        players[args.player_id].total_score += players[args.player_id].current_score;\n        players[args.player_id].current_score = 0; \n\n        // Deciding the winner\n        if (players[args.player_id].total_score &gt;= 60) {\n            io.emit(\"winner\", activePlayer);\n        }\n        else {\n            activePlayer = (activePlayer + 1) % 2;\n            NotifyScores(activePlayer, 0);\n        }\n      }\n      ...\n    }\n });\n</code></pre>\n<p>This can be updated on the client side.</p>\n<h4><code>public/script.js</code></h4>\n<pre><code>socket.on(\"winner\", (winner) =&gt; {\n    playing = false;\n    document.querySelector(`.player--${winner}`).classList.add('player--winner');\n    document.querySelector(`.player--${winner}`).classList.remove('player--active');\n});\n</code></pre>\n<p>This is it, now you can play the game with your friend.\nYou can always host it online using ngrok.\nNow what's left is adding more features like adding an in-game chat, viewers, a gaming room and much more.\nAlso, there is a need to clean the code.</p>\n<h2>Conclusion</h2>\n<p>In this blog, we witnessed the working of a WebSocket app using Socket.io.\nWe focused on establishing a connection and sending messages from server to client as well as from client to server.\nThere are 3 ways in which the messages can be communicated, <code>io.emit()</code>, <code>socket.emit()</code> and <code>socket.broadcast.emit()</code>.\n<code>io.emit()</code> is used for broadcasting the message to all the clients.\n<code>socket.broadcast.emit()</code> broadcasts the message to all the clients except the active client.\n<code>socket.emit()</code> is for server to client or client to server messaging.\nFor receiving the messages we implement a listener, that triggers on receiving a specific event as defined by us.\nWe also saw some special events like <code>connection</code> and <code>disconnect</code>.</p>\n",
            "enclosure": {},
            "categories": []
        },
        {
            "title": "Automate your local environment setup using dev containers",
            "pubDate": "2023-01-08 19:06:49",
            "link": "https://blog-schwiftycold.firebaseapp.com/blog/2023-01-08-devcontainers/",
            "guid": "https://blog-schwiftycold.firebaseapp.com/blog/2023-01-08-devcontainers/",
            "author": "",
            "thumbnail": "",
            "description": "Now you don't need to waste any time helping a friend to contribute to your new project. Using dev containers, you can automate the process of setting up your local environment in seconds.",
            "content": "\n<h2>Index</h2>\n<table>\n<thead><tr>\n<th>Topic</th>\n<th>Description</th>\n</tr></thead>\n<tbody>\n<tr>\n<td><a href=\"https://blog-schwiftycold.firebaseapp.com/#problem\">Problem</a></td>\n<td>Automate the set up process for the projects</td>\n</tr>\n<tr>\n<td><a href=\"https://blog-schwiftycold.firebaseapp.com/#dev-containers\">Dev Containers</a></td>\n<td>What are dev containers?</td>\n</tr>\n<tr>\n<td><a href=\"https://blog-schwiftycold.firebaseapp.com/#dev-container-custom-configurations\">Dev container custom configurations</a></td>\n<td>Configuring a dev container as per your needs.</td>\n</tr>\n<tr>\n<td><a href=\"https://blog-schwiftycold.firebaseapp.com/#open-your-container-in-vscode\">Open your container in VSCode</a></td>\n<td>How to open a dev container in <code>VSCode</code>.</td>\n</tr>\n<tr>\n<td><a href=\"https://blog-schwiftycold.firebaseapp.com/#open-your-container-in-a-browser\">Open your container in a browser</a></td>\n<td>Opening dev container in <code>github.dev </code>text editor.</td>\n</tr>\n<tr>\n<td><a href=\"https://blog-schwiftycold.firebaseapp.com/#creating-a-dev-container-using-templates-available\">Creating a dev container using templates available</a></td>\n<td>\n<code>Java</code> and <code>Postgres</code> (as service) dev container set up.</td>\n</tr>\n<tr>\n<td><a href=\"https://blog-schwiftycold.firebaseapp.com/#adding-fish-terminal-to-the-dev-container\">Adding fish terminal to the dev container</a></td>\n<td>Using <code>fish</code> shell and setting the default shell for a container.</td>\n</tr>\n<tr>\n<td><a href=\"https://blog-schwiftycold.firebaseapp.com/#oh-my-zsh-with-powerlevel10k\">oh-my-zsh with powerlevel10k</a></td>\n<td>Using <code>zsh</code> along with <code>oh-my-zsh</code> and <code>powerlevel10k</code>.</td>\n</tr>\n<tr>\n<td><a href=\"https://blog-schwiftycold.firebaseapp.com/#make-the-dev-container-distributable\">Make the dev container distributable</a></td>\n<td>You should not add personal changes to source control. Use external script to modify the dev container configurations.</td>\n</tr>\n<tr>\n<td><a href=\"https://blog-schwiftycold.firebaseapp.com/#conclusion\">Conclusion</a></td>\n<td>Final words</td>\n</tr>\n</tbody>\n</table>\n<h2>Problem</h2>\n<p>I use a dual-booted ROG with <strong>Ubuntu 22.10</strong> and <strong>Windows 10</strong> for development, along with a <strong>MacBook Air 2020</strong>.\nEvery time I begin a new project that makes use of a new tech stack, I am required to take some time to set up the environment on all 3 machines.\nAnd sometimes, my laziness compels me to use only one machine instead of setting up other ones.\nThe issue is that I don't want to go through the same setup process again and again.</p>\n<p>The same problem exists on a bigger scale too.\nIn <a href=\"https://www.greyorange.com/\">GreyOrange</a>, I've seen my supervisors spending a lot of time helping freshers set up their new laptops.\nAnd even after following the setup guide on confluence, it took me a lot of time to build my first project successfully.\nImagine helping 300 freshers set up their laptops for a particular project.</p>\n<p>Plus, the building process can be different for different Operating Systems.\nWouldn't it be great to use a system that your CICD platform uses?</p>\n<p>This problem can be solved using <strong>dev containers</strong> that only require <code>Docker</code> and <code>git</code> installed on your machine.</p>\n<h2>Dev containers</h2>\n<p>Dev containers are just Docker containers that are fully equipped with the necessary tech stacks and tools to begin developing.\nSimply establish a connection with your container and begin writing code.\nAdditionally, a version control program like <code>git</code> may be used to share this setup.\nSo, anyone with a computer may run these Docker containers and begin developing them.\nGitHub codespace uses dev container configurations to set up their cloud development environment.\nYou can also customize your dev container using the <code>Dockerfile</code> and <code>docker-compose</code> configurations.</p>\n<h2>Dev container custom configurations</h2>\n<p>A simple dev container configuration has a <code>.devcontainer</code> directory consisting of <code>devcontainer.json</code> and a <code>Dockerfile</code>.\nThe <code>devcontainer.json</code> contains all the configurations used by the container while <code>Dockerfile</code> contains the instruction to create a Docker container.\nYou can also use an image from the Docker hub directly by using the <code>image</code> attribute.</p>\n<h4><code>Directory structure</code></h4>\n<pre><code>.devcontainer\n Dockerfile\n devcontainer.json\n</code></pre>\n<h4><code>devcontainer.json</code></h4>\n<pre><code>{\n\t\"name\": \"Hello DevContainer\",   // This is the name of the container\n    \n    // build defines the image configurations\n    // Instead of building you can use `image` attribute to provide the docker image hosted on Docker hub\n    \"build\": {  \n        \"dockerfile\": \"Dockerfile\", // This is the Dockerfile located at '.devcontainer/Dockerfile'\n        \"context\": \"..\",            // This is where the project lies\n        \"args\": {                   // Anything supplied to Dockerfile as an argument\n            \"PYTHON_VERSION\": \"3.9\" \n        }\n    },\n\n\t// Features to add to the dev container. More info: https://containers.dev/features.\n\t// \"features\": {},\n\n\t// Use 'forwardPorts' to make a list of ports inside the container available locally.\n\t// \"forwardPorts\": [],\n\n\t// Will run these after the container is created\n    // Generally used to install the dependencies\n\t// \"postCreateCommand\": \"pip install -r requirements.txt\",\n\n    // You can use root as remoteUser but it's not advisable to do so\n\t\"remoteUser\": \"root\"\n}\n</code></pre>\n<h4><code>Dockerfile</code></h4>\n<pre><code>ARG PYTHON_VERSION\nFROM python:${PYTHON_VERSION}.0-slim\n# And other setup instructions\n</code></pre>\n<h2>Open your container in VSCode</h2>\n<p>To connect to a dev container using <a href=\"https://code.visualstudio.com/\">VSCode</a>, you need to install <a>Dev Containers</a> extension.\nYou can open your current directory in a dev container by clicking <code>F1</code> and selecting <code>Dev Containers: Reopen in Container</code>.\nYou can also choose <code>Dev Containers: Rebuild and Reopen in Container</code> in case you want to build the Docker image once again.</p>\n<p>&lt;!image/&gt;</p>\n<p>This will take some time in building the container and installing the tools.\nOnce completed, you can verify the connection by looking at the left end of the <a href=\"https://code.visualstudio.com/\">VSCode</a> window.\nIt should display <code>Dev Container: &lt;Container name&gt; @ &lt;Operating system&gt;</code></p>\n<p>&lt;!image/&gt;</p>\n<blockquote>\n<p>Note: The extensions are gone. For your dev container, you'll need to add your extensions once again.</p>\n</blockquote>\n<p>You can also check the python version to confirm if it's installed properly.</p>\n<pre><code>&gt; root@2a35d5d14816:/workspaces/devcontainers# python --version\nPython 3.9.0\n</code></pre>\n<h2>Open your container in a browser</h2>\n<p>You can also use the power of VSCode from your browser using <a href=\"https://github.com/github/dev\">github.dev</a> editor.\nYou need to host your project on GitHub for this to work.\nLet's do that.</p>\n<p>First, you will need to add git features in <code>devcontainer.json</code> file.\nThis will enable us to use <code>git</code> and <code>github cli</code> from inside the container.\nYou can also enable <code>ssh</code> using the <code>sshd</code> feature.\nA list of available features can be found at <a href=\"https://containers.dev/features\">containers.dev/features</a>.</p>\n<pre><code>    ...\n    \"features\": {\n        \"ghcr.io/devcontainers/features/git:1\": {},\n        \"ghcr.io/devcontainers/features/github-cli:1\": {},\t\n        \"ghcr.io/devcontainers/features/sshd:1\": {}\n    },\n    ...\n</code></pre>\n<p>Then rebuild the container by pressing <code>F1</code> -&gt; <code>Rebuild Container</code>.\nNow you can use <code>git</code> and <code>gh</code> inside your container.</p>\n<pre><code>&gt; git init\n&gt; git add . \n&gt; git commit -m \"Init\"\n&gt; gh auth login # Here you can create a new ssh key for your container\n&gt; gh repo create hello-devcontainer --public --source . --push\n Created repository GO-Shubham-Kumar/hello-devcontainer on GitHub\n Added remote git@github.com:GO-Shubham-Kumar/hello-devcontainer.git\nEnumerating objects: 5, done.\nCounting objects: 100% (5/5), done.\nDelta compression using up to 4 threads\nCompressing objects: 100% (4/4), done.\nWriting objects: 100% (5/5), 924 bytes | 71.00 KiB/s, done.\nTotal 5 (delta 0), reused 0 (delta 0)\nTo github.com:GO-Shubham-Kumar/hello-devcontainer.git\n * [new branch]      HEAD -&gt; master\nBranch 'master' set up to track remote branch 'master' from 'origin'.\n Pushed commits to git@github.com:GO-Shubham-Kumar/hello-devcontainer.git\n</code></pre>\n<p>The above procedure creates a new repository and pushes the project commits to it.\nI can see my project at <a href=\"https://github.com/GO-Shubham-Kumar/hello-devcontainer\">https://github.com/GO-Shubham-Kumar/hello-devcontainer</a>.</p>\n<p>Now we have our code on GitHub.\nIt's time to open a devcontainer using gitub.dev.\nFor this simply visit your GitHub repo and change the URL from <code>github.com</code> to <code>github.dev</code>.\nThis will launch a new VSCode-style window with all your tools available.</p>\n<p>&lt;!image/&gt;</p>\n<blockquote>\n<p>Note: github.dev is a lightweight editor which does not support a terminal. If you want to use the terminal, you should switch to GitHub Codespaces.</p>\n</blockquote>\n<h2>Creating a dev container using templates available</h2>\n<p>Above, I showed you how to configure a dev container using a test repository.\nNow, I'll show you how to use available templates to run services like <code>PostgreSQL</code> along with your container.</p>\n<p>In <a href=\"https://www.greyorange.com/\">GreyOrange</a>, we have a project that uses <code>SpringBoot</code> and <code>PostgreSQL</code>.\nWe need a list of databases to run the test case for this project.\nLet's try to configure our dev container to have a <code>PostgreSQL</code> service along with the databases required.</p>\n<p>Summary of things we need.</p>\n<ol>\n<li>Java 8 and Maven</li>\n<li>PostgreSQL 9.6</li>\n<li>Populate the databases</li>\n</ol>\n<p>You can visit <a href=\"https://containers.dev/templates.html\">container templates</a> to search for a required template.\nWe will use, <a href=\"https://github.com/devcontainers/templates/tree/main/src/java-postgres\">Java &amp; Postgres</a> template.\nThis template allows us to configure the Java version, package manager and PostgreSQL version.</p>\n<blockquote>\n<p>In case your template isn't available, you will need to create your <code>Dockerfile</code> &amp; <code>docker-compose</code> files.</p>\n</blockquote>\n<p>From your <a href=\"https://code.visualstudio.com/\">VSCode</a> editor,\nClick <code>F1</code> -&gt; <code>Dev Containers: Add Dev Container Configuration Files...</code>.\nClick on  <code>Show All Definitions</code> and select <code>Java &amp; PostgreSQL</code>.\nI'm using version <code>8-bullseye</code> with Maven so I'll select them.\nAfter a few moments, <a href=\"https://code.visualstudio.com/\">VSCode</a> will create a <code>.devcontainer</code> directory with <code>Dockerfile</code>, <code>docker-compose.yml</code> &amp; <code>devcontainer.json</code>.</p>\n<p>Looking at the generated code it seems like we require some modifications for this to work properly.</p>\n<ol>\n<li>\n<p>We want to initialize the databases as per the <code>database_creator.sql</code>.\nThis file contains all the databases we require to run the test cases.\nThis can be done by adding an entry to the <code>db</code> -&gt; <code>volume</code> as shown in the <code>docker-compose.yml</code> file.</p>\n</li>\n<li>\n<p>The Java image should be <code>java:8-bullseye</code>, not <code>java:0-8-bullseye</code> which was generated by the generator.</p>\n</li>\n<li>\n<p>We need <code>Postgres:9.6</code>, the generated one point to the latest image.</p>\n</li>\n</ol>\n<h4><code>devcontainer.json</code></h4>\n<pre><code>// For format details, see https://aka.ms/devcontainer.json. For config options, see the\n// README at: https://github.com/devcontainers/templates/tree/main/src/java-postgres\n{\n\t\"name\": \"Java &amp; PostgreSQL\",\n\t\"dockerComposeFile\": \"docker-compose.yml\",\n\t\"service\": \"app\",\n\t\"workspaceFolder\": \"/workspaces/${localWorkspaceFolderBasename}\"\n}\n</code></pre>\n<h4><code>Dockerfile</code></h4>\n<pre><code># Changed java:0-8-bullseye to java:8-bullseye\nFROM mcr.microsoft.com/devcontainers/java:8-bullseye\n\nARG INSTALL_MAVEN=\"true\"\nARG MAVEN_VERSION=\"\"\n\nARG INSTALL_GRADLE=\"false\"\nARG GRADLE_VERSION=\"\"\n\nRUN if [ \"${INSTALL_MAVEN}\" = \"true\" ]; then su vscode -c \"umask 0002 &amp;&amp; . /usr/local/sdkman/bin/sdkman-init.sh &amp;&amp; sdk install maven \\\"${MAVEN_VERSION}\\\"\"; fi \\\n    &amp;&amp; if [ \"${INSTALL_GRADLE}\" = \"true\" ]; then su vscode -c \"umask 0002 &amp;&amp; . /usr/local/sdkman/bin/sdkman-init.sh &amp;&amp; sdk install gradle \\\"${GRADLE_VERSION}\\\"\"; fi\n\n</code></pre>\n<h4><code>docker-compose.yml</code></h4>\n<pre><code>version: '3.8'\n\nvolumes:\n  postgres-data:\n\nservices:\n  app:\n    container_name: javadev\n    build: \n      context: .\n      dockerfile: Dockerfile\n    environment:\n      # NOTE: POSTGRES_DB/USER/PASSWORD should match values in db container\n        POSTGRES_PASSWORD: postgres\n        POSTGRES_USER: postgres\n        POSTGRES_DB: postgres\n        # We will populate the database using database_creator.sql\n        # POSTGRES_HOSTNAME: postgresdb \n\n    volumes:\n      - ../..:/workspaces:cached\n    command: sleep infinity\n\n    # Runs app on the same network as the database container, allows \"forwardPorts\" in devcontainer.json function.\n    network_mode: service:db\n\n    # Use \"forwardPorts\" in **devcontainer.json** to forward an app port locally. \n    # (Adding the \"ports\" property to this file will not forward from a Codespace.)\n\n  db:\n    container_name: postgresdb\n    image: postgres:9.6\n    restart: always\n    volumes:\n      # Run the below script as an initialization script \n      - ../misc/database_creator.sql:/docker-entrypoint-initdb.d/database_creator.sql\n      # We no longer require this\n      # - postgres-data:/var/lib/postgresql/data\n    environment:\n      # NOTE: POSTGRES_DB/USER/PASSWORD should match values in app container\n      POSTGRES_PASSWORD: postgres\n      POSTGRES_USER: postgres\n      # We will create our databases using misc/database_creator.sql\n      # POSTGRES_DB: postgres\n\n    # Add \"forwardPorts\": [\"5432\"] to **devcontainer.json** to forward PostgreSQL locally.\n    # (Adding the \"ports\" property to this file will not forward from a Codespace.)\n</code></pre>\n<p>The above configurations will run <code>PostgreSQL</code> along with our app.\nYou can also view this combination in Docker Desktop.</p>\n<p>&lt;!image/&gt;</p>\n<blockquote>\n<p>Note: Here I haven't used <code>git</code> and <code>gh</code> features because the image <code>java:8-bullseye</code> already comes with these tools.</p>\n</blockquote>\n<p>Let's verify the <code>Java</code> and <code>Maven</code> versions.</p>\n<pre><code>&gt; java -version\nopenjdk version \"1.8.0_352\"\nOpenJDK Runtime Environment (Temurin)(build 1.8.0_352-b08)\nOpenJDK 64-Bit Server VM (Temurin)(build 25.352-b08, mixed mode)\n&gt; mvn -version\nApache Maven 3.8.7 (b89d5959fcde851dcb1c8946a785a163f14e1e29)\nMaven home: /usr/local/sdkman/candidates/maven/current\nJava version: 1.8.0_352, vendor: Temurin, runtime: /usr/local/sdkman/candidates/java/8.0.352-tem/jre\nDefault locale: en_US, platform encoding: UTF-8\nOS name: \"linux\", version: \"5.15.49-linuxkit\", arch: \"aarch64\", family: \"unix\"\n</code></pre>\n<p>Let's compile the project.</p>\n<pre><code>&gt; mvn clean install\n...\n[INFO] BUILD SUCCESS\n[INFO] ------------------------------------------------------------------------\n[INFO] Total time:  35:03 min\n[INFO] Finished at: 2023-01-08T00:05:51Z\n[INFO] ------------------------------------------------------------------------\n</code></pre>\n<p>It took a while but the build was successful.</p>\n<h2>Adding fish terminal to the dev container</h2>\n<p>Right now, the dev container is using <code>bash</code> as it's the default shell.\nThere are a lot of shells that come along with this image.</p>\n<pre><code>&gt; echo $SHELL\n/bin/bash\n\n&gt; cat /etc/shells\n/bin/sh\n/bin/bash\n/bin/rbash\n/bin/dash\n/bin/zsh\n/usr/bin/zsh\n</code></pre>\n<p>Let's try installing <a href=\"https://fishshell.com/\">fish</a> terminal which is not available by default.\n<code>fish</code> can be installed using the features option in <code>devcontainer.json</code>.\nA list of features is available <a href=\"https://containers.dev/features\">here</a>.</p>\n<p>Modify the <code>devcontainer.json</code> as shown below and rebuild the container.</p>\n<h4><code>devcontainer.json</code></h4>\n<pre><code>{\n    ...\n    \"features\": {\n\t\t\"ghcr.io/meaningful-ooo/devcontainer-features/fish:1\": {}\n\t}\n}\n</code></pre>\n<p>Now, you can find <code>fish</code> in the list of available shells. And this feature also sets <code>fish</code> as your default terminal.</p>\n<pre><code>&gt; cat /etc/shells\n/bin/sh\n/bin/bash\n/bin/rbash\n/bin/dash\n/bin/zsh\n/usr/bin/zsh\n/usr/bin/fish   # This is it\n</code></pre>\n<p>&lt;!image/&gt;</p>\n<p>If you don't want to set <code>fish</code> as your default terminal, you can specify the default terminal in <code>devcontainer.json</code> file as follows.\nThis will set <code>bash</code> as your default container.</p>\n<h4><code>devcontainer.json</code></h4>\n<pre><code>{\n  ...\n\t\"settings\": { \n\t\t\"terminal.integrated.defaultProfile.linux\": \"bash\", \n\t\t\"terminal.integrated.profiles.linux\": { \n\t\t\t\"bash\": { \n\t\t\t\t\"path\": \"bash\" \n\t\t\t\t} \n\t\t\t} \n\t\t}\n  ...\n}\n</code></pre>\n<h2>oh-my-zsh with powerlevel10k</h2>\n<p><code>fish</code> is a great shell.\nBut I want the dev container to feel like my local machine that has <code>oh-my-zsh</code> with <a href=\"https://github.com/romkatv/powerlevel10k\">powerlevel10k</a>.\nAt the time of writing this blog, I wasn't able to find any features to install these things.\nSo I went with configuring the <code>Dockerfile</code> itself to include these features.</p>\n<blockquote>\n<p>Disclaimer: This is a very bad idea to directly change a source controlled dev container file as per your preference</p>\n</blockquote>\n<p>This image already comes with <a href=\"https://ohmyz.sh/\">oh-my-zsh</a>. <a href=\"https://github.com/romkatv/powerlevel10k\">Powerlevel10k</a> is a custom theme that I'll need to install on my own.\nSame goes for <a href=\"https://github.com/zsh-users/zsh-syntax-highlighting\">zsh-syntax-highlighting</a> and <a href=\"https://github.com/zsh-users/zsh-autosuggestions\">zsh-autosuggestions</a> plugins.</p>\n<h4><code>Dockerfile</code></h4>\n<pre><code>...\n# Install powerlevel10k\nRUN git clone --depth=1 https://github.com/romkatv/powerlevel10k.git /home/vscode/.oh-my-zsh/custom/themes/powerlevel10k\n# Install zsh-autosuggestions\nRUN git clone https://github.com/zsh-users/zsh-autosuggestions /home/vscode/.oh-my-zsh/custom/plugins/zsh-autosuggestions\n# Install zsh-syntax-highlighting\nRUN git clone https://github.com/zsh-users/zsh-syntax-highlighting.git /home/vscode/.oh-my-zsh/custom/plugins/zsh-syntax-highlighting\n# This is my local machine's theme file \nCOPY .p10k.zsh /home/vscode\n# This is the modified zsh configuration\nCOPY .zshrc /home/vscode\n</code></pre>\n<p><code>.p10k.zsh</code> contains the <a href=\"https://github.com/romkatv/powerlevel10k\">powerlevel10k</a> configurations.\n<code>.zshrc</code> contains the <code>zsh</code> configurations.\nThese files are located inside the home directory.\nI just copied them to <code>.devcontainer</code> for an easy transfer to the dev container.</p>\n<pre><code>&gt; tree -a .devcontainer\n.devcontainer\n .p10k.zsh\n .zshrc\n Dockerfile\n devcontainer.json\n docker-compose.yml\n</code></pre>\n<blockquote>\n<p>If you don't have the <code>.p10k.zsh</code>, you can skip the COPY step while building the container. Once inside the container, you can run <code>p10k configure</code> to generate the <code>.p10k.zsh</code> file. Then the contents of this file can be copied to <code>.devcontainer/.p10k.zsh</code> for future builds.</p>\n</blockquote>\n<p>Below is my <code>.zshrc</code> file. You can verify the <code>ZSH_THEME</code> is set as <code>powerlevel10k/powerlevel10k</code>.\nI am just using 3 plugins. The <code>git</code> plugin is available by default. <code>zsh-autosuggestions</code> and <code>zsh-syntax-highlighting</code> were downloaded from GitHub.</p>\n<pre><code># Enable Powerlevel10k instant prompt. Should stay close to the top of ~/.zshrc.\n# Initialization code that may require console input (password prompts, [y/n]\n# confirmations, etc.) must go above this block; everything else may go below.\nif [[ -r \"${XDG_CACHE_HOME:-$HOME/.cache}/p10k-instant-prompt-${(%):-%n}.zsh\" ]]; then\n  source \"${XDG_CACHE_HOME:-$HOME/.cache}/p10k-instant-prompt-${(%):-%n}.zsh\"\nfi\n\nexport ZSH=\"$HOME/.oh-my-zsh\"\n\nZSH_THEME=\"powerlevel10k/powerlevel10k\"\n\nplugins=(\n\tgit\n\tzsh-autosuggestions\n\tzsh-syntax-highlighting\n)\n\nsource $ZSH/oh-my-zsh.sh\n\n# To customize prompt, run `p10k configure` or edit ~/.p10k.zsh.\n[[ ! -f ~/.p10k.zsh ]] || source ~/.p10k.zsh\n</code></pre>\n<p>&lt;!image/&gt;</p>\n<blockquote>\n<p>If there are multiple developers with their shell preference, modification of <code>Dockerfile</code> for a specific need is not a very good idea.</p>\n</blockquote>\n<h2>Make the dev container distributable</h2>\n<p>You should not change a source controlled <code>Dockerfile</code> to satisfy your need.\nThe current <code>Dockerfile</code> contains the changes I made to suit my preference.\nI like <code>zsh</code> so I did some customization to Docker to support my theming style.\nIf I commit these changes to the remote, this might become a problem for another developer who prefers some other shell or theme.\nPlus, if everyone starts making changes to the dev container configuration, this might result in merge conflicts.</p>\n<p>In scenarios like this, you can use the git <code>skip-worktree</code> feature.\nAfter cloning the base repo, you don't want to make any commits on the dev container.\nSo you can tell git to ignore tracking the <code>.devcontainer</code> directory.</p>\n<pre><code># Tell git to stop tracking these files\n&gt; git update-index --assume-unchanged .devcontainer/*\n\n# View the files which are not being tracked\n&gt; git ls-files -v | grep \"h\" | grep dev\nh .devcontainer/Dockerfile\nh .devcontainer/devcontainer.json\nh .devcontainer/docker-compose.yml\n</code></pre>\n<p>Now you can freely configure the dev container as per your need.</p>\n<blockquote>\n<p>Actually, this is not the best way to deal with this problem at all, as this will ignore the <code>.devcontainer</code> directory even in the case of <code>git pull</code>.\nAnd if you do the reverse using <code>git update-index --no-assume-unchanged .devcontainer/*</code>, you will need to commit or discard your changes before proceeding.</p>\n</blockquote>\n<p>The best way I can think of to deal with this problem is by keeping the personal installations away from the git repository.</p>\n<p>For this, let's create an external script that will install <code>powerlevel10k</code> and other plugins using the <code>docker</code> commands.</p>\n<p>First, let's move our <code>.zshrc</code> and <code>.p10k.zsh</code> to a new directory out of our repository.\nI'm creating a hidden directory in the <code>home</code> directory for this.</p>\n<pre><code>&gt; mkdir ~/.personal-devcontainer \n&gt; mv .devcontainer/.zshrc ~/.personal-devcontainer\n&gt; mv .devcontainer/.p10k.zsh ~/.personal-devcontainer\n</code></pre>\n<p>Also, let's reset the <code>devcontainer.json</code>, <code>Dockerfile</code> and <code>docker-compose.yml</code> to the unbiased version.</p>\n<h4><code>devcontainer.json</code></h4>\n<pre><code>// For format details, see https://aka.ms/devcontainer.json. For config options, see the\n// README at: https://github.com/devcontainers/templates/tree/main/src/java-postgres\n{\n\t\"name\": \"Java &amp; PostgreSQL\",\n\t\"dockerComposeFile\": \"docker-compose.yml\",\n\t\"service\": \"app\",\n\t\"workspaceFolder\": \"/workspaces/${localWorkspaceFolderBasename}\"\n}\n</code></pre>\n<h4><code>Dockerfile</code></h4>\n<pre><code># Changed java:0-8-bullseye to java:8-bullseye\nFROM mcr.microsoft.com/devcontainers/java:8-bullseye\n\nARG INSTALL_MAVEN=\"true\"\nARG MAVEN_VERSION=\"\"\n\nARG INSTALL_GRADLE=\"false\"\nARG GRADLE_VERSION=\"\"\n\nRUN if [ \"${INSTALL_MAVEN}\" = \"true\" ]; then su vscode -c \"umask 0002 &amp;&amp; . /usr/local/sdkman/bin/sdkman-init.sh &amp;&amp; sdk install maven \\\"${MAVEN_VERSION}\\\"\"; fi \\\n    &amp;&amp; if [ \"${INSTALL_GRADLE}\" = \"true\" ]; then su vscode -c \"umask 0002 &amp;&amp; . /usr/local/sdkman/bin/sdkman-init.sh &amp;&amp; sdk install gradle \\\"${GRADLE_VERSION}\\\"\"; fi\n\n</code></pre>\n<h4><code>docker-compose.yml</code></h4>\n<pre><code>version: '3.8'\n\nvolumes:\n  postgres-data:\n\nservices:\n  app:\n    container_name: javadev\n    build: \n      context: .\n      dockerfile: Dockerfile\n    environment:\n      # NOTE: POSTGRES_DB/USER/PASSWORD should match values in db container\n        POSTGRES_PASSWORD: postgres\n        POSTGRES_USER: postgres\n        POSTGRES_DB: postgres\n        # We will populate the database using database_creator.sql\n        # POSTGRES_HOSTNAME: postgresdb \n\n    volumes:\n      - ../..:/workspaces:cached\n    command: sleep infinity\n\n    # Runs app on the same network as the database container, allows \"forwardPorts\" in devcontainer.json function.\n    network_mode: service:db\n\n    # Use \"forwardPorts\" in **devcontainer.json** to forward an app port locally. \n    # (Adding the \"ports\" property to this file will not forward from a Codespace.)\n\n  db:\n    container_name: postgresdb\n    image: postgres:9.6\n    restart: always\n    volumes:\n      # Run the below script as an initialization script \n      - ../misc/database_creator.sql:/docker-entrypoint-initdb.d/database_creator.sql\n      # We no longer require this\n      # - postgres-data:/var/lib/postgresql/data\n    environment:\n      # NOTE: POSTGRES_DB/USER/PASSWORD should match values in app container\n      POSTGRES_PASSWORD: postgres\n      POSTGRES_USER: postgres\n      # We will create our databases using misc/database_creator.sql\n      # POSTGRES_DB: postgres\n\n    # Add \"forwardPorts\": [\"5432\"] to **devcontainer.json** to forward PostgreSQL locally.\n    # (Adding the \"ports\" property to this file will not forward from a Codespace.)\n</code></pre>\n<p>Now, let's create a bash script inside <code>~/.personal-devcontainer</code> that will change the container configuration as per our preference (personal changes).</p>\n<pre><code>&gt; cd ~/.personal-devcontainer\n&gt; touch install.sh\n&gt; chmod +x install.sh\n&gt; vim install.sh\n</code></pre>\n<h4><code>install.sh</code></h4>\n<pre><code># This will work on a running dev container\nCONTAINER_NAME=$1\ndocker exec $CONTAINER_NAME git clone --depth=1 https://github.com/romkatv/powerlevel10k.git /home/vscode/.oh-my-zsh/custom/themes/powerlevel10k\ndocker exec $CONTAINER_NAME git clone https://github.com/zsh-users/zsh-autosuggestions /home/vscode/.oh-my-zsh/custom/plugins/zsh-autosuggestions\ndocker exec $CONTAINER_NAME git clone https://github.com/zsh-users/zsh-syntax-highlighting.git /home/vscode/.oh-my-zsh/custom/plugins/zsh-syntax-highlighting\ndocker cp ~/.personal-devcontainer/.p10k.zsh $CONTAINER_NAME:/home/vscode/.p10k.zsh\ndocker cp ~/.personal-devcontainer/.zshrc $CONTAINER_NAME:/home/vscode/.zshrc\ndocker exec $CONTAINER_NAME chsh -s $(which zsh)\n</code></pre>\n<p>Now you can just run <code>./install.sh javadev</code> to install the preferences to the javadev container (which is our development container name).\nAs I'll keep changing the script depending on my projects, I'll publish the installation scripts repo to my <a href=\"https://github.com/GO-Shubham-Kumar/personal-devcontainer.git\">git repo</a>.</p>\n<p>There are 2 problems with this method -</p>\n<ol>\n<li>You will require to run the script on every rebuild.</li>\n<li>You cannot directly use this script online with <code>github.dev</code>.</li>\n</ol>\n<p>For easier access, let's create an alias for the script. It will take a container name as a parameter and will configure the shell as required.</p>\n<h4><code>.zshrc</code></h4>\n<pre><code>...\n# configure-personal-dev-container javadev\nfunction configure-personal-dev-container {\n\tif [[ -z $1 ]]; then \n\t\techo Please provide a container name\n\t\texit 1\n\telse\n\t\t~/.personal-devcontainer/install.sh $1\n\tfi\n}\n...\n</code></pre>\n<p>Now, I can run <code>configure-personal-dev-container javadev</code> from anywhere.</p>\n<h2>Conclusion</h2>\n<p>Dev container helps us to reduce the setup time for a project.\nWe started with creating a simple dev container configuration.\nWe used the power of dev containers to open the project in <code>VSCode</code> and <code>github.dev</code>.\nThen we created a complex dev container configuration consisting of <code>Java</code> &amp; <code>Maven</code> with <code>PostgreSQL</code> installation and database creation for an ongoing project at <a href=\"https://www.greyorange.com/\">GreyOrange</a>.\nWe tried to compile our project inside the dev container which worked as expected.\nThen we moved to configure the <code>shell</code> inside the dev container.\nWe tried <code>fish</code> and <code>zsh</code> with <code>powerlevel10k</code> which is what I use locally.\nWe also learned to configure the dev container for personal development without disturbing the source-controlled setup configurations.</p>\n",
            "enclosure": {},
            "categories": []
        },
        {
            "title": "A note on creating dynamic link previews for your website or blog",
            "pubDate": "2022-12-23 07:53:55",
            "link": "https://blog-schwiftycold.firebaseapp.com/blog/2022-12-23-info-link-previews/",
            "guid": "https://blog-schwiftycold.firebaseapp.com/blog/2022-12-23-info-link-previews/",
            "author": "",
            "thumbnail": "",
            "description": "This is a little blog post on how to dynamically add a preview of your website while posting on social media",
            "content": "\n<p>import BlogImageWithContext from \"../../components/BlogImageWithContext.astro\";\nimport BlogImage from \"../../components/BlogImage.astro\";\nimport LinkPreviewDemo from \"../../images/posts/2022/info-link-previews/link-preview-demo.png\";\nimport OpenGrapgDebugging from \"../../images/posts/2022/info-link-previews/opengraph-debugging.png\";\nimport OpenGrapgDebuggingPreview from \"../../images/posts/2022/info-link-previews/opengraph-debugging-preview.png\";\nimport FlaskAppPreviewImage from \"../../images/posts/2022/info-link-previews/flask-app-preview-image.png\";\nimport SlackPreview from \"../../images/posts/2022/info-link-previews/link-preview-on-slack.png\";\nimport FinalPreview from \"../../images/posts/2022/info-link-previews/final-preview.png\";</p>\n<h2>Index</h2>\n<table>\n<thead><tr>\n<th>Topic</th>\n<th>Description</th>\n</tr></thead>\n<tbody>\n<tr>\n<td><a href=\"https://blog-schwiftycold.firebaseapp.com/#general-intoduction\">General Introduction</a></td>\n<td>Understanding link previews</td>\n</tr>\n<tr>\n<td><a href=\"https://blog-schwiftycold.firebaseapp.com/#defining-meta-tags\">Defining meta tags</a></td>\n<td>Open-graph and Twitter mea tags</td>\n</tr>\n<tr>\n<td><a href=\"https://blog-schwiftycold.firebaseapp.com/#viewing-and-debuggig\">Viewing and Debuggig</a></td>\n<td>Using opengraph.xyz to view your previews</td>\n</tr>\n<tr>\n<td><a href=\"https://blog-schwiftycold.firebaseapp.com/#make-it-dynamic\">Make it dynamic</a></td>\n<td>Create a custom server to generate images</td>\n</tr>\n<tr>\n<td><a href=\"https://blog-schwiftycold.firebaseapp.com/#hosting-the-server-on-heroku\">Hosting the server on Heroku</a></td>\n<td>Deploy the server on Heroku and test its working</td>\n</tr>\n<tr>\n<td><a href=\"https://blog-schwiftycold.firebaseapp.com/#conclusion\">Conclusion</a></td>\n<td>Final words</td>\n</tr>\n</tbody>\n</table>\n<h2>General Introduction</h2>\n<p>Using social networking apps, you can add a preview to the URL you share.\nAs a result, the reader can get a decent notion of the contents.</p>\n<p>&lt;!image-context/&gt;</p>\n<h2>Hosting the server on <a href=\"https://dashboard.heroku.com/\">Heroku</a>\n</h2>\n<p>The Flask <code>dev</code> server won't work with <a href=\"https://dashboard.heroku.com/\">Heroku</a> deployments.\nLet's use <a href=\"https://gunicorn.org/\">Gunicorn</a> for our deployments.</p>\n<p>Create a new file with the name <code>Procfile</code> with no extensions and modify its contents as below.</p>\n<h4><code>Procfile</code></h4>\n<pre><code>web: gunicorn gettingstarted.wsgi\nweb: gunicorn app:app\n</code></pre>\n<p>The first line tells <a href=\"https://dashboard.heroku.com/\">Heroku</a> to use <a href=\"https://gunicorn.org/\">Gunicorn</a> as server.\nThe second line tells<a href=\"https://gunicorn.org/\">Gunicorn</a> to run the app.\n<a href=\"https://gunicorn.org/\">Gunicorn</a> uses the syntax <code>gunicorn filename:app_name</code> to start a server.</p>\n<p>Also, update the <code>requirements.txt</code> file by adding <code>gunicorn</code>.</p>\n<h4><code>requirements.txt</code></h4>\n<pre><code>Flask\npillow\nrequests\ngunicorn\n</code></pre>\n<p>Now you can use <a href=\"https://devcenter.heroku.com/articles/heroku-cli\">Heroku CLI</a> to host the app following the below steps.</p>\n<p>You can install the Heroku CLI using the instructions <a href=\"https://devcenter.heroku.com/articles/heroku-cli\">here</a>.</p>\n<pre><code># First you need git (Heroku uses git)\n&gt; git init\n&gt; git add .\n&gt; git commit -m \"My first link preview\"\n\n# Login to your account\n&gt; heroku Login\n\n# This will create an app and add a remote to your git\n&gt; heroku create -a &lt;some-unique-name&gt;\n\n&gt; git remote -v\nheroku https://git.heroku.com/&lt;some-unique-name&gt;.git\n\n# To deploy just push your code to Heroku remote\n&gt; git push heroku main \n</code></pre>\n<p>Let's check it on <a href=\"https://opengraph.xyz/\">opengraph.xyz</a>.</p>\n<p>&lt;!image/&gt;</p>\n<p>This is exactly what we wanted.\nNow, we can create multiple HTML files with different titles and our server will generate a preview accordingly.</p>\n<p>This was a small demo of generating link previews.\nYou can further customize your server to generate previews based on title, description, publish date and much more.</p>\n<h2>Conclusion</h2>\n<p>A dynamic link preview can be generated by hosting a server and pointing our og/twitter image to it.\nHere we used Flask to create a GET route that provides us with the image.\nYou can add more flexibility/features by using <code>html2img</code> module for generating images from HTML directly.\nWe also saw the process of hosting a Gunicorn server on Heroku.\nThis proved to be more useful in testing our deployment on <a href=\"https://opengraph.xyz/\">opengraph.xyz</a> as reverse proxy had some problems.</p>\n",
            "enclosure": {},
            "categories": []
        },
        {
            "title": "Combining multiple git repositories into a single repository and retaining all the commit histories",
            "pubDate": "2022-12-13 18:30:00",
            "link": "https://blog-schwiftycold.firebaseapp.com/blog/2022-12-14-monorepo-migration/",
            "guid": "https://blog-schwiftycold.firebaseapp.com/blog/2022-12-14-monorepo-migration/",
            "author": "",
            "thumbnail": "",
            "description": "Initially, you thought these projects required a separate version control system, but you were wrong. It seems like they are all interdependent. Let's see how you can merge them together without losing anything.",
            "content": "\n<h2>Contents</h2>\n<table>\n<thead><tr>\n<th>Topics</th>\n<th>Description</th>\n</tr></thead>\n<tbody>\n<tr>\n<td><a href=\"https://blog-schwiftycold.firebaseapp.com/#the-problem\">The Problem</a></td>\n<td>Merge multiple git repositories into one while preserving all histories</td>\n</tr>\n<tr>\n<td><a href=\"https://blog-schwiftycold.firebaseapp.com/#a-bit-about-git\">A bit about git</a></td>\n<td>Introduction to Blob, Tree and Commits</td>\n</tr>\n<tr>\n<td><a href=\"https://blog-schwiftycold.firebaseapp.com/#rewriting-history\">Rewriting history</a></td>\n<td>Uusing filter-branch to rewrite history. Moving a file inside a directory while retaining git histories</td>\n</tr>\n<tr>\n<td><a href=\"https://blog-schwiftycold.firebaseapp.com/#merging-multiple-repos-into-one\">Merging multiple repos into one</a></td>\n<td>A step by step guide for combining two git repositories into a larger parent repo while preserving histories</td>\n</tr>\n<tr>\n<td><a href=\"https://blog-schwiftycold.firebaseapp.com/#merging-using-shopsys-monorepo-tools\">Merging using Shopsys monorepo-tools</a></td>\n<td>Merging two git repos into a larger parent repo using monorepo tools by shopsys</td>\n</tr>\n<tr>\n<td><a href=\"https://blog-schwiftycold.firebaseapp.com/#conclusion\">Conclusion</a></td>\n<td>Final words</td>\n</tr>\n</tbody>\n</table>\n<h2>The Problem</h2>\n<p>In our company, we had multiple maven projects that were dependent on each other.\nAnd for every project, we maintained a separate <code>git</code> repository hosted on Bitbucket.</p>\n<p>Due to a certain use-case, we wanted to migrate all our Bitbucket repos to GitHub.\nAnd to avoid any circular dependency issues, we also wanted to combine all these related repositories into a mono-repo system.</p>\n<p>The idea is to have a parent git repository that will contain all these repositories as a subdirectory.</p>\n<blockquote>\n<p>Things described here will work for any project using git version control, irrespective of the hosting platform.</p>\n</blockquote>\n<h4><code>Folder structure</code></h4>\n<pre><code>parent\n  POM.XML        # Our new parent POM\n  .git           # This is our new git that will contain all the histories\n  repo1         \n     POM.XML\n     ...\n     .git       # This git will be removed\n  repo2\n     POM.XML\n     ...\n     .git       # This git will be removed\n  repo3\n      POM.XML\n      ...\n      .git       # This git will be removed\n</code></pre>\n<p>Combining the repos is the easier part.\nAs we were using Maven, all we required was to create a master <code>POM.XML</code> that would contain all the sub-repos as submodules.</p>\n<blockquote>\n<p>Again, the steps defined here are not dependent on Maven or any other package manager.</p>\n</blockquote>\n<p>The difficult part was to retain all the histories, which I'll explain in this blog.\nBecause each repository has it's own <code>.git</code>, removing this will delete all the associated histories.\nAfter which we won't be able to identify the code changes, commits, and most importantly, the branches.</p>\n<p>At <a href=\"https://www.greyorange.com/\">GreyOrange</a>, we maintain a <code>develop</code> branch and certain <code>release</code> branches for each repo.\nWe were required to merge all the respective commits on develop and release branches as well.\nWe wanted to merge their histories so that we could have all our commits on the same branch as they were originally.\nWe also had many feature branches, but they were mostly independent, so merging wasn't required for them.</p>\n<h2>A bit about git</h2>\n<p>When you run <code>git init</code>, it creates a new <code>.git</code> directory. This directory contains everything <code>git</code> needs to do it's magic.</p>\n<h3>\n<code>objects</code> directory</h3>\n<p>The blobs, trees, and commits are all stored in the <code>objects</code> directory within <code>.git</code>. These are the 3 basic elements that define all the functionality of <code>git</code>.</p>\n<p>Blobs are a type of data structure that contains compressed information related to the changed files.\nEach time you run <code>git add . </code>, a new blob is created inside the <code>objects</code> directory.\nYou can view a blob's contents using <code>git cat-file</code>.</p>\n<blockquote>\n<p>Blobs are just the changed files hashed using <code>SHA-1</code></p>\n</blockquote>\n<pre><code>&gt; ls .git/objects\ninfo/     pack/\n\n# Create a file with it's content as test\n&gt; echo \"test\" &gt;&gt; test.txt\n&gt; ls .\n    test.txt \n\n# adding changes creates blob of the changed files\n&gt; git add .\n&gt; ls .git/objects\n0c/   info/    pack/      # 0c is the directory containing the changes\n\n&gt; ls .git/objects/0c\n2c5f41c83de09587dfe46d5a5382eddf5bb77f\n# The complete hash of the blob is 0c2c5f41c83de09587dfe46d5a5382eddf5bb77f\n# Note: The first 2 letters are the directory name\n\n# You can also view the contents of the blob using cat-file utility\n&gt; git cat-file blob 0c2c5f41c83de09587dfe46d5a5382eddf5bb77f\ntest\n</code></pre>\n<p>Multiple blobs are combined together to form a tree data structure.\nGenerally, trees are created during a commit.\nBut you can run <code>git write-tree</code> to generate a tree of recently added blobs.\nThese trees are also stored inside the <code>objects</code> directory.</p>\n<blockquote>\n<p>Trees are like the directories containing the blobs and other trees.</p>\n</blockquote>\n<pre><code>&gt; git write-tree    # This outputs the hash associated with the tree generated\n56bac5fc9c69776a5c67daa2225ef9b2e1edd4f6\n\n# Trees are stored in the same manner\n&gt; ls .git/objects/56\nbac5fc9c69776a5c67daa2225ef9b2e1edd4f6\n\n# You can also view the content of a tree file\n# It contains a reference to the blobs or trees\n&gt; git cat-file -p 56bac5fc9c69776a5c67daa2225ef9b2e1edd4f6\n100644 blob 0c2c5f41c83de09587dfe46d5a5382eddf5bb77f    test.txt\n# 0c2c5f... is the hash of the blob created above\n</code></pre>\n<p>So a tree represents the state of the system.\nEach commit is just 1 tree which is hashed and stored along with other information like author &amp; date.\nThey are also stored in the same manner.</p>\n<pre><code>&gt; git commit -m \"initial commit\"\n\n&gt; ls .git/objects\n0c/   52/   56/   info/    pack/    # 52 is newly generated directory\n\n&gt; git cat-file -p 5203c0048b4795669114fcdb261dc5bb4e77a54f\ntree 56bac5fc9c69776a5c67daa2225ef9b2e1edd4f6       # This is the hash of the tree we created above\nauthor Shubham Kumar &lt;unresolved.shubham@gmail.com&gt; 1670872587 +0530\ncommitter Shubham Kumar &lt;unresolved.shubham@gmail.com&gt; 1670872587 +0530\n</code></pre>\n<p>A commit just contains the recent tree hash and inforamtion on author, time and committer.</p>\n<blockquote>\n<p>It's not possible to distinguish a blob, a tree and a commit just by looking at the objects directory.</p>\n</blockquote>\n<h3>\n<code>refs</code> directory</h3>\n<p>The <code>refs</code> directory contains the reference to commits.\nThere are 2 types of reference - <code>branch</code> and <code>tags</code>.\nTags identify a unique commit while branch points to the latest child along the commit tree.</p>\n<pre><code>&gt; ls .git/refs/heads\nmaster               # There is just one branch right now\n\n# Let's create a new branch and see what happens\n&gt; git checkout -b dev\nSwitched to a new branch 'dev'\n\n&gt; ls .git/ref/heads\ndev   master        # Now you can see both the branches\n\n&gt; cat .git/ref/heads/dev\n5203c0048b4795669114fcdb261dc5bb4e77a54f \n# Points to the latest commit. This is the exact same hash of the commit we created above. \n\n&gt; git commit -m \"commit to dev\" --allow-empty\n[dev 9d6d972] commit to dev\n\n# Creating a new commit changes the contents of the active branch.\n&gt; cat .git/ref/heads/dev\n9d6d972066b774e89343e57f2eb053559bf3f22c \n</code></pre>\n<h3>\n<code>logs</code> directory</h3>\n<p>This contains the history of every branch.\nEverytime you change the branch using <code>git checkout &lt;branch-name&gt;</code> or update the tip, <code>logs/HEAD</code> is updated.\n<code>logs/refs/heads</code> contains the history of commits for a particular branch.\nThis is a safety net. You can easily retrieve your work even after a rebase.</p>\n<pre><code># To view the logs/HEAD\n&gt; git reflog\n9d6d972 (HEAD -&gt; dev) HEAD@{0}: checkout: moving from master to dev\n5203c00 (master) HEAD@{1}: checkout: moving from dev to master\n9d6d972 (HEAD -&gt; dev) HEAD@{2}: commit: commit to dev\n5203c00 (master) HEAD@{3}: checkout: moving from master to dev\n5203c00 (master) HEAD@{4}: commit (initial): initial commit\n</code></pre>\n<h2>Rewriting History</h2>\n<p><a href=\"https://git-scm.com/docs/git-filter-branch\"><code>filter-branch</code></a> can be used to rewrite the entire history of a git repository.\nThis will create a new blob, tree and commit for everything once again.\nRewriting history using the <code>filter-branch</code> does not hamper the commit data.\nHashes will change but the entire inforamtion about the author, committer, etc remains unchanged.</p>\n<pre><code># For all the commits do this -&gt; mkdir nested; mv test.txt nested/test.txt\n&gt; git filter-branch --tree-filter 'mkdir nested; mv test.txt nested/test.txt' HEAD\n\n&gt; tree .\n.\n nested\n     test.txt\n\n&gt; git cat-file -p d1950a39cbb6b1212e47d0c5be3b19e023051671  # This I got by looking inside the `objects` dir\ntree 3ea8798ffe7147c04d66c9bef3ac5109ad6e80b8   # New tree reference\nauthor Shubham Kumar &lt;unresolved.shubham@gmail.com&gt; 1670872587 +0530     # Commit time remained unchanged\ncommitter Shubham Kumar &lt;unresolved.shubham@gmail.com&gt; 1670872587 +0530  \n\ninitial commit\n\n# Let's see what's inside this tree\n# We moved our 'test.txt' file inside the 'nested' directory. This creates a new tree. \n&gt; git cat-file -p 3ea8798ffe7147c04d66c9bef3ac5109ad6e80b8      \n040000 tree 2b297e643c551e76cfa1f93810c50811382f9117    nested  \n\n# Inside the 'nested' directory is our file.\n&gt; git cat-file -p 2b297e643c551e76cfa1f93810c50811382f9117\n100644 blob 9daeafb9864cf43055ae93beb0afd6c7d144bfa4    test.txt\n\n# And inside the file is the content. \n&gt; git cat-file blob 9daeafb9864cf43055ae93beb0afd6c7d144bfa4\ntest\n</code></pre>\n<h2>Merging multiple repos into one</h2>\n<p>Merging multiple repositories requires us to take the objects and other entities from one repo and copy it to some other repo.\nFor the sake of explaining, I just created 2 test repos - <a href=\"https://github.com/unresolvedcold/test-repo-1\">test-repo-1</a> and <a href=\"https://github.com/unresolvedcold/test-repo-2\">test-repo-2</a>.\nBoth contains a single 'README.MD' file with some content. We'll try merging them into a monorepo system with a parent git and these repos as subdirectories.</p>\n<pre><code>&gt; cat test-repo-1/README.md\n# test-repo-1\n\nThis is a test repository.\n\n&gt; git log --reflog  # To view all the commits\ncommit f23153640c79ef57849be2baac14f6daa7b96a1c (HEAD -&gt; main, origin/main, origin/HEAD)\nAuthor: Shubham Kumar &lt;35415266+UnresolvedCold@users.noreply.github.com&gt;\nDate:   Tue Dec 13 11:02:33 2022 +0530\n\n    Updated README.MD\n\ncommit 527ee4e25e8a145448bf799412c369c6cbc8e934\nAuthor: Shubham Kumar &lt;35415266+UnresolvedCold@users.noreply.github.com&gt;\nDate:   Tue Dec 13 11:01:57 2022 +0530\n\n    Initial commit\n</code></pre>\n<pre><code>&gt; cat test-repo-2/README.md\n# test-repo-2\n\nThis is some other test repo.\n\n&gt; git log --reflog  # To view all the commits\ncommit 69ed70f51232f81a29a537fb6534e91d1d1ac9c2 (HEAD -&gt; main, origin/main, origin/HEAD)\nAuthor: Shubham Kumar &lt;35415266+UnresolvedCold@users.noreply.github.com&gt;\nDate:   Tue Dec 13 11:04:00 2022 +0530\n\n    Updated README.MD &lt;repo-2&gt;\n\ncommit fe71fde928e17f4adbba0c637a6afc4c503f455a\nAuthor: Shubham Kumar &lt;35415266+UnresolvedCold@users.noreply.github.com&gt;\nDate:   Tue Dec 13 11:03:12 2022 +0530\n\n    Initial commit\n</code></pre>\n<p>We have 2 git repos with 2 commits on <code>main</code> branch. Let's try merging them.</p>\n<h4><code>Current folder structure</code></h4>\n<pre><code>.\n test-repo-1\n   .git        # This will be removed\n   README.md\n test-repo-2\n     .git        # This will be removed\n     README.md\n</code></pre>\n<h4><code>New folder structure</code></h4>\n<pre><code>parent\n .git            # New git that will contain all our histories\n test-repo-1\n   README.md\n test-repo-2\n     README.md\n</code></pre>\n<p>We will create a new git repository.\nAnd include the remotes of the repository we want to merge.\nWe want to fetch everything from these repos. This is done by calling <code>git fetch --all</code>.\nThen, one by one, we can create a new subdirectory for each repository.\nAnd at last, we can manipulate the git history into thinking the codes were present in the subdirectory from the beginning.</p>\n<pre><code>&gt; git init  # Create a new repository\n\n# Fetch the repos we want to merge \n&gt; git remote add repo1 git@github.com:UnresolvedCold/test-repo-1.git\n&gt; git remote add repo2 git@github.com:UnresolvedCold/test-repo-2.git\n&gt; git fetch --all\nFetching repo1\nremote: Enumerating objects: 6, done.\nremote: Counting objects: 100% (6/6), done.\nremote: Compressing objects: 100% (2/2), done.\nremote: Total 6 (delta 0), reused 0 (delta 0), pack-reused 0\nUnpacking objects: 100% (6/6), 1.23 KiB | 180.00 KiB/s, done.\nFrom github.com:UnresolvedCold/test-repo-1\n * [new branch]      main       -&gt; repo1/main\nFetching repo2\nremote: Enumerating objects: 6, done.\nremote: Counting objects: 100% (6/6), done.\nremote: Compressing objects: 100% (2/2), done.\nremote: Total 6 (delta 0), reused 0 (delta 0), pack-reused 0\nUnpacking objects: 100% (6/6), 1.24 KiB | 424.00 KiB/s, done.\nFrom github.com:UnresolvedCold/test-repo-2\n * [new branch]      main       -&gt; repo2/main\n</code></pre>\n<p>Now we have everything updated. We can start our merging process.\nCheckout the branch of the first repository and modify the index file such that repo1 files are inside the <code>repo1</code> directory.</p>\n<pre><code># Let's start with the first repo main branch\n&gt; git checkout --detach repo1/main\n\n&gt; git ls-files -s   # This is how the file is presently \n100644 142ee887e3184237ac17f918498bda405eeb5fc1 0\tREADME.md\n\n# We want to move README.md to repo1/README.md\n&gt; git ls-files -s | sed \"s-\\t\\\"*-&amp;repo1/-\"      # We want to do this for all the commits\n100644 142ee887e3184237ac17f918498bda405eeb5fc1 0\trepo1/README.md\n\n# After doing this for all the commits we will update the index file\n&gt; git filter-branch --index-filter '\n     git ls-files -s | sed \"s-\\t\\\"*-&amp;repo1/-\" | \n     GIT_INDEX_FILE=$GIT_INDEX_FILE.new git update-index --index-info &amp;&amp;  mv \"$GIT_INDEX_FILE.new\" \"$GIT_INDEX_FILE\"' \n\n&gt; ls -a \n./    ../    .git/    repo1/   # A new directory called repo1 is created\n\n&gt; ls repo1/\nREADME.md\n\n# We can see our the first repo contents are inside repo1 directory\n&gt; cat README.md\n# test-repo-1\n\nThis is a test repository.\n</code></pre>\n<p>The above looks good. At least we can see the files related to repo1 in <code>repo1</code> subdirectory.\nLet's explore the history. It should be a tree called <code>repo1</code> with <code>README.md</code> inside it.</p>\n<pre><code># Let's see the git commit list\n&gt; git log --pretty=format:\"%H\"\n495e27b84df8e4e6d297b37a03f1c6c0a5fdcc73    # This is the latest commit\n8a5f4f3805be9604b1f96b03df5cf1092363ba91\n\n# Let's view the histories for the latest commit\n&gt; git cat-file -p 495e27b84df8e4e6d297b37a03f1c6c0a5fdcc73\ntree 38eccdd305b8e410a1993391031e948e664d7b1d\nparent 8a5f4f3805be9604b1f96b03df5cf1092363ba91\nauthor Shubham Kumar &lt;35415266+UnresolvedCold@users.noreply.github.com&gt; 1670909553 +0530\ncommitter GitHub &lt;noreply@github.com&gt; 1670909553 +0530\n\nUpdated README.MD%\n&gt; git cat-file -p 38eccdd305b8e410a1993391031e948e664d7b1d\n040000 tree d83102ebb26f925b8e087c821e49ca910f1750a6\trepo1   # Has a directory called repo1\n&gt; git cat-file -p d83102ebb26f925b8e087c821e49ca910f1750a6\n100644 blob 142ee887e3184237ac17f918498bda405eeb5fc1\tREADME.md   # repo1 has README.MD\n\n# Let's quickly check the second commit id\n&gt; git cat-file -p 8a5f4f3805be9604b1f96b03df5cf1092363ba91 | grep tree\ntree 0dc5c3d1b78ed4dcd97120149441a8e3a8d6aefa\n&gt; git cat-file -p 0dc5c3d1b78ed4dcd97120149441a8e3a8d6aefa\n040000 tree cc92c678191b4d3c4731f5d0f9b212532316ef8c\trepo1     # Check\n&gt; git cat-file -p cc92c678191b4d3c4731f5d0f9b212532316ef8c\n100644 blob 6a642ba8d3e31ba9a02606da98dd4a73a2d554e2\tREADME.md # Check\n</code></pre>\n<p>We can see that both the files and commit histories were successfully migrated to repo1.\nNow we can do the same process for repo2.\nIf you have more than one branch, you'll need to repeat the process for each one.</p>\n<p>But first, we must delete the original references in order to create a new one.\nAnd we also want to merge the previous references with the new ones.\nSo let's save the current HEAD and delete the references.</p>\n<pre><code># Our current HEAD (latest commit in our case)\n&gt; git rev-parse HEAD\n495e27b84df8e4e6d297b37a03f1c6c0a5fdcc73\n\n# Save the current HEAD for merging later on\n&gt; REPO1=$(git rev-parse HEAD)\n\n# Delete the original references\n&gt; git for-each-ref --format=\"%(refname)\" refs/original/\nrefs/original/HEAD\n&gt; git update-ref -d refs/original/HEAD\n&gt; git reset --hard\n</code></pre>\n<p>We are now ready to merge <code>repo2</code>. Again we'll need to reset the HEAD before merging.\nWe'll create a new branch called <code>main</code> and merge both repos to it one by one.</p>\n<pre><code>&gt; git checkout --detach repo2/main\n&gt; git filter-branch --index-filter '\n     git ls-files -s | sed \"s-\\t\\\"*-&amp;repo2/-\" | \n     GIT_INDEX_FILE=$GIT_INDEX_FILE.new git update-index --index-info &amp;&amp;  mv \"$GIT_INDEX_FILE.new\" \"$GIT_INDEX_FILE\"' \n\n# Save the reference\n&gt; REPO2=$(git rev-parse HEAD)\n\n# Reset HEAD (again before merging)\n&gt; git update-ref -d refs/original/HEAD\n&gt; git reset --hard\n</code></pre>\n<p>Now we can begin our merging process.</p>\n<pre><code>&gt; git checkout -b main\n&gt; git merge --no-commit -q $REPO1 $REPO2 --allow-unrelated-histories\n# If no merge conflicts then you can just commit it else you'll need to resolve merge conflicts\n&gt; git commit -m \"Migrated\"\n\n&gt; tree\n.\n repo1\n   README.md\n repo2\n     README.md\n\n&gt; git log --oneline\nd0b5806 (HEAD -&gt; main) Migrated\n074ef5b Updated README.MD &lt;repo-2&gt;\n025ec51 Initial commit\n495e27b Updated README.MD\n8a5f4f3 Initial commit\n</code></pre>\n<p>This is the fundamental method for merging two repositories into one while retaining all commit histories.\nNow, of course, we never want to manually redo this for all the repos and all the branches.\nA script would be helpful.</p>\n<h2>Merging using Shopsys monorepo-tools</h2>\n<p>And as they say,</p>\n<blockquote>\n<p>Don't reinvent the wheel.</p>\n</blockquote>\n<p>There are numerous such merger tools available online, we used <a href=\"https://github.com/shopsys/monorepo-tools\">monorepo-tools by shopsys</a>.\nThere were several tweaks we did for our use case. One was that by default this tool only merges the <code>master</code> branch.\nAnd here we are trying to merge the <code>main</code> branch so, we'll need to update the <code>shopsys</code> repo to do this.</p>\n<blockquote>\n<p>Under the hood, monorepo-tools is using the same commands described above.</p>\n</blockquote>\n<pre><code># Replace master by main in monorepo files\n# Run this inside the shopsys monorepo-tools directory\n&gt; for f in $(find *.sh); do c=$(cat $f | sed 's/master/main/g'); echo $c&gt;$f ;done\n\n# sed 's/master/main/g' $f &gt; $f should work on Linux based system \n# but on MAC it just deletes all the contents of my file.\n</code></pre>\n<p>Let's see how you can merge 2 repos using shopsys monorepo-tools now.</p>\n<pre><code># First clone the shopsys repo\n&gt; git clone git@github.com:shopsys/monorepo-tools.git\n\n# Create a new repo\n&gt; mkdir parent\n&gt; cd parent\n&gt; git init\n\n# Add the remotes and fetch\n&gt; git remote add repo1 git@github.com:UnresolvedCold/test-repo-1.git\n&gt; git remote add repo2 git@github.com:UnresolvedCold/test-repo-2.git\n&gt; git fetch --all\n\n# Run Shopsys mono-repo tool\n&gt; ../monorepo-tools/monorepo_build.sh repo1 repo2\n</code></pre>\n<p>This is how easy it is to merge multiple repos using shopsys monorepo-tools.\nThere are a variety of other things that you try with this tool like splitting a repo into multiple repos (reverse of what we did here).\nNot exploring this here.</p>\n<p>Let's verify if everything worked !!!</p>\n<pre><code>&gt; tree .\n.\n repo1\n   README.md\n repo2\n     README.md   # Both the repositories are under their sub-directories\n\n&gt; git log --oneline\nad3f765 (HEAD -&gt; main) merge multiple repositories into a monorepo\n074ef5b Updated README.MD &lt;repo-2&gt;\n025ec51 Initial commit\n495e27b Updated README.MD\n8a5f4f3 Initial commit\n\n# Explore the latest commit\n&gt; git cat-file -p ad3f765 | grep tree\ntree 630e5e1c57649efcd9929baa790768927783659e\n&gt; git cat-file -p 630e5e1c57649efcd9929baa790768927783659e\n040000 tree d83102ebb26f925b8e087c821e49ca910f1750a6\trepo1   # Check\n040000 tree 7801ea011e82b38c3f3de145571ad75536d5bd5c\trepo2   # Check\n&gt; git cat-file -p d83102ebb26f925b8e087c821e49ca910f1750a6\n100644 blob 142ee887e3184237ac17f918498bda405eeb5fc1\tREADME.md   # Check\n&gt; git cat-file blob 142ee887e3184237ac17f918498bda405eeb5fc1\n# test-repo-1\n\nThis is a test repository.\n&gt; git cat-file -p 7801ea011e82b38c3f3de145571ad75536d5bd5c\n100644 blob 18c9019fe2e7d5e8151db4cb5f1d10307c8547ec\tREADME.md   # Check\nThis is a test repository.\n&gt; git cat-file blob 18c9019fe2e7d5e8151db4cb5f1d10307c8547ec\n# test-repo-2\n\nThis is some other test repo.\n\n# We can also verify commits 074ef5b and 495e27b to see if README.md is inside their respective folders\n&gt; git cat-file -p 074ef5b | grep tree\ntree 057f76982f62d051ed841e43c09536d3f3c61980\n&gt; git cat-file -p 057f76982f62d051ed841e43c09536d3f3c61980\n040000 tree 7801ea011e82b38c3f3de145571ad75536d5bd5c\trepo2     # Check\n&gt; git cat-file -p 7801ea011e82b38c3f3de145571ad75536d5bd5c\n100644 blob 18c9019fe2e7d5e8151db4cb5f1d10307c8547ec\tREADME.md # Check\n\n&gt; git cat-file -p 495e27b | grep tree\ntree 38eccdd305b8e410a1993391031e948e664d7b1d\n&gt; git cat-file -p 38eccdd305b8e410a1993391031e948e664d7b1d\n040000 tree d83102ebb26f925b8e087c821e49ca910f1750a6\trepo1     # Check\n&gt; git cat-file -p d83102ebb26f925b8e087c821e49ca910f1750a6\n100644 blob 142ee887e3184237ac17f918498bda405eeb5fc1\tREADME.md # Check\n</code></pre>\n<p>Everything seems perfect.</p>\n<h2>Conclusion</h2>\n<p>The merging of multiple repositories into a monorepo structure can be accomplished by rewriting <code>git</code> histories.\nThis can be done manually by fetching a repo and tweaking the git index to believe it belongs to a subdirectory rather than the main repo itself.\nAnother way is by using open-source tools like <a href=\"https://github.com/shopsys/monorepo-tools\">shopsys monorepo tools</a> with which merging can be done in a few steps.\nMore tweaks can be done with <code>monorepo-tools</code> to merge multiple branches and create a higher level <code>POM</code> inside every commit.</p>\n",
            "enclosure": {},
            "categories": []
        },
        {
            "title": "A simple blogging site for personal use",
            "pubDate": "2022-12-04 18:30:00",
            "link": "https://blog-schwiftycold.firebaseapp.com/blog/2022-12-05-create-your-own-custom-blog/",
            "guid": "https://blog-schwiftycold.firebaseapp.com/blog/2022-12-05-create-your-own-custom-blog/",
            "author": "",
            "thumbnail": "",
            "description": "it's an adventure to control your contents and hosting platforms",
            "content": "\n<p>This is how I quickly hosted the platform where this blog exists that you are reading right now.</p>\n<blockquote>\n<p>There isn't a way to post comments as of now. You'll need to ping me on social platforms.</p>\n</blockquote>\n<p>All I required was a text editor with dev container support (like <strong>VS Code</strong>), <strong>Docker</strong> and <strong>Github account</strong>.\nFor deployment there are many static site hosting platforms. I am going with <strong><a href=\"https://firebase.google.com/\">firebase hosting</a></strong> for deploying my site.\nAs per the advice of one of my close friends, <a href=\"https://mobile.twitter.com/guruxkancharla\">Guruvardhan</a> this is built using Astro (MDX pages for blogging).</p>\n<h2>Understanding the flow</h2>\n<blockquote>\n<p>There are many templates available in astro library. The one I'm using is <a href=\"https://github.com/lancerossdev/astro-basic-blog\">Lance Ross astro basic template</a>.</p>\n</blockquote>\n<h3>Folder structure</h3>\n<pre><code>src\n components\n images\n layouts\n    BlogLayout.astro\n    Layout.astro\n pages\n     404.astro\n     index.astro\n     posts\n        but-why-tho.mdx\n        how-to-deploy.mdx\n        markdown-styling.mdx\n     posts.astro\n     rss.xml.ts\n</code></pre>\n<p><code>src/pages/posts</code> is where the blog posts live. All these <code>mdx</code> files are different blogs.</p>\n<p><code>src/pages/*.astro</code> contains the main view components of the website. The layouts are defined at <code>src/layouts</code>.</p>\n<p><code>rss.xml.ts</code> is the configuration for my blog's RSS feed. You can subscribe using Feedly or any other newsreader app.</p>\n<h3>Publishing the blog</h3>\n<p>The defaut branch for my blog repository is called <code>publish</code>.\nAny pushes to this will automatically trigger the rebuilding and publishing of the website.</p>\n<p>So I create a new branch for each of my new blogs and when I'm finished, I just merge the changes to <code>publish</code> branch.\nThis triggers the <code>deploy-firebase</code> workflow which pushes the build to firebase hosting.\nAnd the updated blog is live within seconds.</p>\n<h2>How to launch your own instance</h2>\n<p>You can always fork my repository <a href=\"https://github.com/UnresolvedCold/blog\">here</a>.\nYou may need to activate GitHub Actions in your repository.\nBetter way is to use <a href=\"https://github.com/lancerossdev/astro-basic-blog\">Lance Ross astro basic template</a>.</p>\n<p>&lt;!image/&gt;</p>\n<p>Click on <code>Use this template</code> -&gt; <code>Create a new repository</code>. This will help you to create a new repository using the template.\nYou can customize your repository as you like.</p>\n<p>For firabse hosting, you'll need to create a new app from their website. Update the <code>.firebasesrc</code> with the name of your app.\nYou'll also need <code>firebase-cli</code> to generate the auth token. Simply run <code>firebase login:ci</code> from your terminal to get this token.\nSave this token as GitHub Action secret as key value with key as 'FIREBASE_TOKEN' and value as the token you generated.</p>\n<h3>Get a custom domain and link it to firebase hosting</h3>\n<ol>\n<li>Create an account on <a href=\"https://blog-schwiftycold.firebaseapp.com/porkbun.com\">Porkbun</a> and buy a domain of your liking.</li>\n<li>Go to firebase and click on hosting. There you can find <code>Add custom domain</code> button.\n&lt;!image/&gt;</li>\n<li>Clicking this will ask you to provide your domain (Example: shubham.codes). Don't mention 'https' or 'www'.</li>\n<li>In the next steps, it will ask you to update <code>A</code> and <code>TXT</code> records with your domain provider which is <a href=\"https://blog-schwiftycold.firebaseapp.com/porkbun.com\">porkbun</a> in our case.\n<ol>\n<li>Go to porkbun and click on the <code>Details</code> button on your domain name.</li>\n<li>Click on DNS record and update the <code>A</code> and <code>TXT</code>.\n&lt;!image/&gt;</li>\n<li>It takes some time to get activated. It's mentioned 24 hours but for me it took around 2 hours.</li>\n</ol>\n</li>\n</ol>\n",
            "enclosure": {},
            "categories": []
        }
    ]
}